{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NURSE_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0h1aG7uIYYjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167975ad-f955-4a9c-c83e-8e41508a364b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BAyahA1u0xIC",
        "outputId": "795939dd-afa1-47ab-afe5-178bdbdd1a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.12.0\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 12.4 MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0\n",
            "  Downloading bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.4.2)\n",
            "Collecting enum34==1.1.10\n",
            "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 79.5 MB/s \n",
            "\u001b[?25hCollecting html5lib==0.9999999\n",
            "  Downloading html5lib-0.9999999.tar.gz (889 kB)\n",
            "\u001b[K     |████████████████████████████████| 889 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==4.0.1\n",
            "  Downloading importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting joblib==1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting Keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.1.2)\n",
            "Collecting Markdown==3.3.4\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting networkx==2.5.1\n",
            "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 64.1 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.17.0\n",
            "  Downloading protobuf-3.17.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 44.2 MB/s \n",
            "\u001b[?25hCollecting python-louvain==0.15\n",
            "  Downloading python-louvain-0.15.tar.gz (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 64.4 MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting threadpoolctl==2.1.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting typing-extensions==3.10.0.0\n",
            "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
            "Collecting Werkzeug==2.0.0\n",
            "  Downloading Werkzeug-2.0.0-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 93.8 MB/s \n",
            "\u001b[?25hCollecting zipp==3.4.1\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Building wheels for collected packages: html5lib, python-louvain\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-py3-none-any.whl size=107234 sha256=c50d6803c680fe930282081ab37b44609e8ee061f64888b722ffa8f20f669edb\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/8d/2f/a9d4c8e58cffde4e60ba7f8aac7495de9bc12cffdc2f947723\n",
            "  Building wheel for python-louvain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-louvain: filename=python_louvain-0.15-py3-none-any.whl size=9413 sha256=a457f5c4f1a3f5596c0027dade11ca17433ee78d391c9f8471ccae0e023139dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/4c/ef/427db8777620f7fa12a6404f1527a39536a812f46a4d1044d3\n",
            "Successfully built html5lib python-louvain\n",
            "Installing collected packages: six, numpy, zipp, typing-extensions, h5py, threadpoolctl, scipy, PyYAML, networkx, Keras-Applications, joblib, importlib-metadata, html5lib, Werkzeug, scikit-learn, python-louvain, protobuf, Markdown, Keras, enum34, bleach, absl-py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.7.0\n",
            "    Uninstalling zipp-3.7.0:\n",
            "      Successfully uninstalled zipp-3.7.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.6.3\n",
            "    Uninstalling networkx-2.6.3:\n",
            "      Successfully uninstalled networkx-2.6.3\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: html5lib\n",
            "    Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: python-louvain\n",
            "    Found existing installation: python-louvain 0.16\n",
            "    Uninstalling python-louvain-0.16:\n",
            "      Successfully uninstalled python-louvain-0.16\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.3.6\n",
            "    Uninstalling Markdown-3.3.6:\n",
            "      Successfully uninstalled Markdown-3.3.6\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: bleach\n",
            "    Found existing installation: bleach 4.1.0\n",
            "    Uninstalling bleach-4.1.0:\n",
            "      Successfully uninstalled bleach-4.1.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.2.4 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-2.2.4 Keras-Applications-1.0.8 Markdown-3.3.4 PyYAML-5.4.1 Werkzeug-2.0.0 absl-py-0.12.0 bleach-1.5.0 enum34-1.1.10 h5py-2.10.0 html5lib-0.9999999 importlib-metadata-4.0.1 joblib-1.0.1 networkx-2.5.1 numpy-1.19.5 protobuf-3.17.0 python-louvain-0.15 scikit-learn-0.24.2 scipy-1.5.4 six-1.16.0 threadpoolctl-2.1.0 typing-extensions-3.10.0.0 zipp-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum",
                  "google",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-tensorboard==1.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvfaryLuboY9",
        "outputId": "2d1ad79b-4edf-4cfe-a673-54a696d67e7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-tensorboard==1.5.1\n",
            "  Downloading tensorflow_tensorboard-1.5.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (1.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (3.17.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (1.19.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (2.0.0)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (1.5.0)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.7/dist-packages (from tensorflow-tensorboard==1.5.1) (0.9999999)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard==1.5.1) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorflow-tensorboard==1.5.1) (3.10.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorflow-tensorboard==1.5.1) (3.4.1)\n",
            "Installing collected packages: tensorflow-tensorboard\n",
            "Successfully installed tensorflow-tensorboard-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge9kjw4dbdBb",
        "outputId": "eb6a5feb-efa3-4bc6-8301-34666c799891"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 29 kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.16.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 88.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a9ea0551307f4d94558f46a681b9a2d0376ed9152a040c7e88934e07ca38e709\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ5Mn-uObxng",
        "outputId": "ecbb0b58-22da-463c-e0ae-444d06afb108"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import csv\n",
        "import collections\n",
        "import networkx as nx\n",
        "import  os\n",
        "import math\n",
        "import operator\n",
        "import random\n",
        "import copy\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "dTUZzRm0WL-_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "#-------------------------Pickle Functions-------------------------------------------------------\n",
        "def loadPickleFile(filepath):\n",
        "\tprint(\"Loading the pickle file from\",filepath,\"...........\")\n",
        "\tpickle_in = open(filepath,\"rb\")\n",
        "\texample_dict = pickle.load(pickle_in)\n",
        "\tprint(\"Loaded the pickle File\")\n",
        "\treturn example_dict\n",
        "\n",
        "def dumpPickleFile(data,filepath):\n",
        "\tpickle_out = open(filepath,\"wb\")\n",
        "\tprint(\"Dumping the Pickle file into \",filepath,\"...........\")\n",
        "\tpickle.dump(data, pickle_out)\n",
        "\tprint(\"Dumped the pickle File\")\n",
        "\tpickle_out.close() \n",
        "\n",
        "#-------------------------JSON Functions-------------------------------------------------------\n",
        "def dumpJsonFile(dictionary,filepath):\n",
        "\tprint(\"Dumping a dictionary to filepath\",filepath,\"...............\")\n",
        "\twith open(filepath,\"w+\") as jsonFile:\n",
        "\t\tjson.dump(dictionary,jsonFile,indent=4,sort_keys =True)\n",
        "\tprint(\"Dumped Successfully\")\n",
        "\n",
        "def loadJsonFile(filepath):\n",
        "\tprint(\"Loading a dictionary to filepath\",filepath,\"...............\")\n",
        "\tdictionary = {}\n",
        "\twith open(filepath) as jsonFile:\n",
        "\t\tdictionary = json.load(jsonFile)\n",
        "\tprint(\"Loaded Successfully\")\n",
        "\treturn dictionary\n",
        "\n",
        "\n",
        "#-------------------------CSV Functions----------------------------------------------------------\n",
        "def appendToCSV(row,filepath):\n",
        "\twith open(filepath,\"a\",buffering = 1) as csvfile:\n",
        "\t\twriter = csv.writer(csvfile)\n",
        "\t\twriter.writerow(row)\n",
        "\n",
        "def openCSVfile(filepath,delimiter = \",\"):\n",
        "\twith open(filepath,\"r\") as csvfile:\n",
        "\t\trows =  csv.reader(csvfile,delimiter = delimiter)\n",
        "\t\treturn list(rows)\n",
        "\n",
        "def csvTojson(csvfile,jsonfile):\n",
        "\trows = []\n",
        "\tgraph = {}\n",
        "\twith open(csvfile,\"r\") as csvfile:\n",
        "\t\trows = csv.reader(csvfile)\n",
        "\t\trows = list(rows)\n",
        "\trows = rows[1:]\n",
        "\tprint(\"Making graph....\")\n",
        "\tfor row in rows:\n",
        "\t\tsource = row[0]\n",
        "\t\ttarget = row[1]\n",
        "\t\tweight = row[2]\n",
        "\t\tif source not in graph:\n",
        "\t\t\tgraph[source] = {}\n",
        "\t\t\tgraph[source][target] = int(weight)\n",
        "\t\telse:\n",
        "\t\t\tgraph[source][target] = int(weight)\n",
        "\tprint(\"Dumping to file.........\")\n",
        "\twith open(jsonfile,\"w\") as jsonfile:\n",
        "\t\tjson.dump(graph,jsonfile)\n",
        "\tprint(\"Dumped json file\")\n",
        "\n",
        "\n",
        "\n",
        "#-------- Dictionrary Functions--------------------------------------------------------------\n",
        "def revertDictionary(dictionary):\n",
        "\tnewDictionary = {}\n",
        "\tprint(\"\")\n",
        "\tfor key in dictionary.keys():\n",
        "\t\tvalue = dictionary[key]\n",
        "\t\tif value in dictionary:\n",
        "\t\t\tprint(\"Cannot Revert Duplicate Values Present....\")\n",
        "\t\t\tprint(\"Returned the same dictionary as it is\")\n",
        "\t\t\treturn dictionary\n",
        "\t\telse:\n",
        "\t\t\tnewDictionary[value] = key\n",
        "\tprint(\"Successfully Reverted the dictionary\")\n",
        "\treturn newDictionary\n",
        "\n",
        "def sortDictionary(dictionary,attribute = \"k\",rev = True):\n",
        "\tod = {}\n",
        "\tif attribute == \"k\":\n",
        "\t\tprint(\"Ordering by key.....\")\n",
        "\t\tod = collections.OrderedDict(sorted(dictionary.items(), key=lambda t: t[0],reverse = rev))  \n",
        "\telif attribute == \"v\":\n",
        "\t\tprint(\"Ordering by Value.....\")\n",
        "\t\tod = collections.OrderedDict(sorted(dictionary.items(), key=lambda t: t[1],reverse = rev))\n",
        "\telse:\n",
        "\t\tprint(\"Invalid attribute\")\n",
        "\treturn od\n",
        "\n",
        "\n",
        "#------------------------OS FUNCTIONS----------------------------------------\n",
        "def get_directory_list(folderpath):\n",
        "    directory_list = []\n",
        "    for root,d_names,f_names in os.walk(folderpath):\n",
        "        for dname in d_names:\n",
        "            if dname.find(\"2019\") != -1:\n",
        "                #print(dname)\n",
        "                directory_list.append(dname)\n",
        "\n",
        "    directory_list.sort()\n",
        "    for d in directory_list:\n",
        "        print(d)\n",
        "    return directory_list\n",
        "\n",
        "def get_file_list(folderpath):\n",
        "    file_list = []\n",
        "    for root,d_names,f_names in os.walk(folderpath):\n",
        "        for fname in f_names:\n",
        "            file_list.append(fname)\n",
        "    file_list.sort()\n",
        "    return file_list"
      ],
      "metadata": {
        "id": "R8w88iMcXf0M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import community\n",
        "import networkx.algorithms\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "# import basic_functions\n",
        "import math\n",
        "\n",
        "\n",
        "def sortEdgesByWeight(graph,reverse = True):\n",
        "\t\"\"\"\n",
        "\tSort Edges by Weight\n",
        "\t\"\"\"\n",
        "\tprint(\"Sorting Edges by Edge Weights .....\")\n",
        "\treturn sorted(graph.edges(data = True),key = lambda x: x[2]['weight'],reverse = reverse)\n",
        "\n",
        "def sortNodesByDegree(graph,weight = \"weight\",reverse = True):\n",
        "\t\"\"\"\n",
        "\tSort Nodes by Degree or Strength\n",
        "\t\"\"\"\n",
        "\tif weight == \"weight\":\n",
        "\t\tprint(\"Sorting Nodes by Strength .....\")\n",
        "\telse:\n",
        "\t\tprint(\"Sorting Nodes by Degree.....\")\n",
        "\treturn sorted(graph.degree(weight = weight),key = lambda x: x[1], reverse = reverse)\n",
        "\n",
        "def getAvgCoreness(list_of_vertices,corenessDict):\n",
        "\t\"\"\"\n",
        "\tGet Average Coreness of the list of vertices\n",
        "\t\"\"\"\n",
        "\tAdd_coreness = 0\n",
        "\tfor v in list_of_vertices:\n",
        "\t\tAdd_coreness += corenessDict[v]\n",
        "\treturn Add_coreness/len(list_of_vertices)\n",
        "\n",
        "\n",
        "def makeSubgraph(G,nodes):\n",
        "\t\"\"\"\n",
        "\tMake Subgraph as Nodes\n",
        "\t\"\"\"\n",
        "\treturn G.subgraph(nodes)\n",
        "\n",
        "def graphInfo(graph,weighted = 2,path_lengths = False):\n",
        "\t\"\"\"\n",
        "\tGive a Basic Analysis of the Graph\n",
        "\tweighted = {0:\"only unweighted\",1:\"only weighted\",else:\"both weighted and unweighted\"}\n",
        "\tpath_lengths = {True:\"\"}\n",
        "\t\"\"\"\n",
        "\tgraph_info = {}\n",
        "\n",
        "\tnfnodes = graph.number_of_nodes()\n",
        "\tnfedges = graph.number_of_edges()\n",
        "\tnfComponents = nx.number_connected_components(graph)\n",
        "\tdensity = nx.density(graph)\n",
        "\n",
        "\tgraph_info = {\n",
        "\t\t\"nfnodes\" : nfnodes,\n",
        "\t\t\"nfedges\" : nfedges,\n",
        "\t\t\"nfComponents\" : nfComponents,\n",
        "\t\t\"density\" : density\n",
        "\t}\n",
        "\n",
        "\tif weighted == 0:\n",
        "\t\tunweighted_size = graph.size(weight = None)\n",
        "\t\tgraph_info['unweighted_size'] = unweighted_size\n",
        "\telif weighted == 1:\n",
        "\t\tweighted_size = graph.size(weight = \"weight\")\n",
        "\t\tgraph_info['weighted_size'] = weighted_size\n",
        "\telse:\n",
        "\t\tunweighted_size = graph.size(weight = None)\n",
        "\t\tweighted_size = graph.size(weight = \"weight\")\n",
        "\t\tgraph_info['unweighted_size'] = unweighted_size\n",
        "\t\tgraph_info['weighted_size'] = weighted_size\n",
        "\n",
        "\tmax_unweighted_node_degree = 0\n",
        "\tmax_weighted_node_degree = 0\n",
        "\n",
        "\tif weighted == 0:\n",
        "\t\tsorted_nodes_by_unweighted_degree = sortNodesByDegree(graph,weight = None,reverse = True)\n",
        "\n",
        "\t\tif nfnodes >= 2:\n",
        "\t\t\tmax_unweighted_node_degree  = sorted_nodes_by_unweighted_degree[0]\n",
        "\t\t\tgraph_info['max_unweighted_node_degree'] = max_unweighted_node_degree\n",
        "\t\n",
        "\telif weighted == 1:\n",
        "\t\tsorted_edges = sortEdgesByWeight(graph)\n",
        "\t\tsorted_nodes_by_weighted_degree = sortNodesByDegree(graph,weight = \"weight\",reverse = True)\n",
        "\t\tmax_edge_weight = None\n",
        "\t\t\n",
        "\t\tif nfedges > 1:\n",
        "\t\t\tmax_edge_weight = sorted_edges[0]\n",
        "\t\t\tgraph_info['max_edge_weight'] = max_edge_weight\n",
        "\n",
        "\t\tif nfnodes >= 2:\n",
        "\t\t\tmax_weighted_node_degree  = sorted_nodes_by_weighted_degree[0]\n",
        "\t\t\tgraph_info['max_weighted_node_degree'] = max_weighted_node_degree\n",
        "\t\n",
        "\telse:\n",
        "\t\tsorted_edges = sortEdgesByWeight(graph)\n",
        "\t\tsorted_nodes_by_weighted_degree = sortNodesByDegree(graph,weight = \"weight\",reverse = True)\n",
        "\t\tsorted_nodes_by_unweighted_degree = sortNodesByDegree(graph,weight = None,reverse = True)\n",
        "\t\tmax_edge_weight = None\n",
        "\t\t\n",
        "\t\tif nfedges > 1:\n",
        "\t\t\tmax_edge_weight = sorted_edges[0] \n",
        "\t\t\tgraph_info['max_edge_weight'] = max_edge_weight\n",
        "\n",
        "\t\tif nfnodes >= 2:\n",
        "\t\t\tmax_unweighted_node_degree  = sorted_nodes_by_unweighted_degree[0]\n",
        "\t\t\tmax_weighted_node_degree  = sorted_nodes_by_weighted_degree[0]\n",
        "\t\t\tgraph_info['max_unweighted_node_degree'] = max_unweighted_node_degree\n",
        "\t\t\tgraph_info['max_weighted_node_degree'] = max_weighted_node_degree\n",
        "\n",
        "\tweighted_avg_path_length = math.inf\n",
        "\tunweighted_avg_path_length = math.inf \n",
        "\n",
        "\tif nfComponents == 1 and path_lengths == False:\n",
        "\t\tweighted_avg_path_length = nx.average_shortest_path_length(graph,weight = \"weight\")\n",
        "\t\tunweighted_avg_path_length = nx.average_shortest_path_length(graph,weight = None)\n",
        "\t\tgraph_info['weighted_avg_path_length'] = weighted_avg_path_length\n",
        "\t\tgraph_info['unweighted_avg_path_length'] = unweighted_avg_path_length\n",
        "\n",
        "\treturn graph_info\n",
        "\n",
        "\n",
        "def makeComponents(G):\n",
        "\t\"\"\"\n",
        "\tMake Component Subgraphs of G\n",
        "\t\"\"\"\n",
        "\tsubgraphs = []\n",
        "\tfor c in nx.connected_components(G):\n",
        "\t\tsubgraphs.append(G.subgraph(c))\n",
        "\treturn subgraphs\n",
        "\n",
        "def allComponentsInfo(graph):\n",
        "\t\"\"\"\n",
        "\tGive all the info of the components\n",
        "\t\"\"\"\n",
        "\tcomponents = makeComponents(graph)\n",
        "\tallCompInfo = {}\n",
        "\tfor component_no,component in enumerate(components):\n",
        "\t\tcompInfo = graph_info(component)\n",
        "\t\tallCompInfo[\"comp_\" + str(component_no)] = compInfo\n",
        "\treturn allCompInfo\n",
        "\n",
        "\n",
        "#--------------------------------Community Functions-----------------------------------------\t\n",
        "def makeCommunitiesSubgraphs(G,partition):\n",
        "\tcommunities = {}\n",
        "\tfor node in partition:\n",
        "\t\tcommunity_no = partition[node]\n",
        "\t\tcommunities[community_no] = [node] if community_no not in communities else communities[community_no].append(node)\n",
        "\t\n",
        "\tfor community_no in communities:\n",
        "\t\tnodes = communities[community_no]\n",
        "\t\tsubGraph = G.subgraph(nodes)\n",
        "\t\tgraphInfo(subGraph)\n",
        "\n",
        "def convert_partition_in_dict_format(partition_fs):\n",
        "\t\"\"\"\n",
        "\tNetworkX returns partition object as an iterator, community module returns dictionary\n",
        "\tFunction convert NetworkX partition object into a community partition dictaionary object\n",
        "\t\"\"\"\n",
        "\tpartition = {}\t\n",
        "\tfor fs_no,fs in enumerate(partition_fs):\n",
        "\t\tcommunity = list(fs)\n",
        "\t\tfor v in community:\n",
        "\t\t\tpartition[v] = fs_no\n",
        "\treturn partition\n",
        "\n",
        "def makeCommunities(graph,weight = True):\n",
        "\t\"\"\"\n",
        "\tUse Louvain Greedy Maximize to maximize Modularity\n",
        "\t\"\"\"\n",
        "\tpartition = None\n",
        "\tif weight == True:\n",
        "\t\tpartition = community.best_partition(graph, weight = 'weight')\n",
        "\telse:\n",
        "\t\tpartition = community.best_partition(graph, weight = None)\n",
        "\treturn partition\n",
        "\n",
        "def makeGreedyCommunities(G,weight = True):\n",
        "\t\"\"\"\n",
        "\tUse Girvan Newman Algorithm to Maximize Modularity\n",
        "\t\"\"\"\n",
        "\tpartition = None\n",
        "\tif weight == True:\n",
        "\t\tpartition = nx.algorithms.community.modularity_max.greedy_modularity_communities(G,weight = \"weight\")\n",
        "\telse:\n",
        "\t\tpartition =  nx.algorithms.community.modularity_max.greedy_modularity_communities(G,weight = None)\n",
        "\treturn partition\n",
        "\n",
        "def makeGNCommunity(graph):\n",
        "\t\"\"\"\n",
        "\tUse Girvan Newman Algorithm to Maximize Modularity\n",
        "\t\"\"\"\n",
        "\tpartition = nx.algorithms.community.centrality.girvan_newman(graph)\n",
        "\treturn partition\n",
        "\n",
        "def makeASYN_LPACommunities(G,weight = True):\n",
        "\t\"\"\"\n",
        "\tUse Asynchronous Label Propogation to Maximize Modularity\n",
        "\t\"\"\"\n",
        "\tpartition = None\n",
        "\tif weight == True:\n",
        "\t\tpartition = nx.algorithms.community.label_propagation.asyn_lpa_communities(G, weight = \"weight\", seed = None)\n",
        "\telse:\n",
        "\t\tpartition = nx.algorithms.community.label_propagation.asyn_lpa_communities(G, weight = None, seed = None)\n",
        "\treturn partition\n",
        "\n",
        "def makeLPACommunities(G,weight = True):\n",
        "\t\"\"\"\n",
        "\tUse Synchronous Label Propogation to Maximize Modularity\n",
        "\t\"\"\"\n",
        "\tpartition = None\n",
        "\tif weight == True:\n",
        "\t\tpartition = nx.algorithms.community.label_propagation.label_propagation_communities(G)\n",
        "\telse:\n",
        "\t\tpartition = nx.algorithms.community.label_propagation.label_propagation_communities(G)\n",
        "\treturn partition\n",
        "\n",
        "def getCommunityPerformance(G,partition):\n",
        "\t\"\"\"\n",
        "\tGet Community Performance of the partition\n",
        "\t\"\"\"\n",
        "\treturn nx.algorithms.community.quality.performance(G, partition)\n",
        "\n",
        "def getCommunityCoverage(G,partition):\n",
        "\t\"\"\"\n",
        "\tGet Community Coverage of the partition\n",
        "\t\"\"\"\n",
        "\treturn nx.algorithms.community.quality.coverage(G, partition)\n",
        "\n",
        "def makeCommunityInducedGraph(graph,partition,weight = True):\n",
        "\t\"\"\"\n",
        "\tGet Community Induced Graph of the partition\n",
        "\t\"\"\"\n",
        "\tinduced_subgraph = None\n",
        "\tif weight == True:\n",
        "\t\tinduced_subgraph = community.induced_graph(partition,graph, weight = 'weight')\n",
        "\telse:\n",
        "\t\tinduced_subgraph = community.induced_graph(partition,graph, weight = None)\n",
        "\treturn induced_subgraph\n",
        "\n",
        "def getModularity(graph,partition,weight = True):\n",
        "\t\"\"\"\n",
        "\tGet Modularity Value for the Partition\n",
        "\t\"\"\"\n",
        "\tmodularity = None\n",
        "\tif weight ==  True:\n",
        "\t\tmodularity = community.modularity(partition,graph,weight = \"weight\")\n",
        "\telse:\n",
        "\t\tmodularity = community.modularity(partition,graph,weight = None)\n",
        "\treturn modularity\n",
        "\n",
        "\n",
        "def convert_to_Pajek(G,filename):\n",
        "\t\"\"\"\n",
        "\tConvert Graph into Pajek Format\n",
        "\t\"\"\"\n",
        "\tprint(\"Writing In pajek Format to \",filename)\n",
        "\tnx.write_pajek(G,\"Data/collusive_users_graph_Pajek.net\",encoding = 'UTF-8')\n",
        "\tprint(\"Written to Pajek\")\n",
        "\n",
        "#----------------------------------PajekFunctions------------------------------------------------\n",
        "\n",
        "def convert_to_Pajek(G,filename):\n",
        "\t\"\"\"\n",
        "\tConvert Graph into Pajek Format\n",
        "\t\"\"\"\n",
        "\tprint(\"Writing In pajek Format to \",filename)\n",
        "\tnx.write_pajek(G,filename,encoding = 'UTF-8')\n",
        "\tprint(\"Written To\",filename,\"in Pajek Format\")\n",
        "\n",
        "def makePajekGraphIDtoNetworkXID(graphfilename):\n",
        "\t\"\"\"\n",
        "\t\"\"\"\n",
        "\trows = openCSVfile(graphfilename,\"\t\")\n",
        "\tPajekGraphID2NetworkxGraphID = {}\n",
        "\tfor row in rows:\n",
        "\t\tif row[0] == \"*edges\":\n",
        "\t\t\tprint(\"GOT edges\")\n",
        "\t\t\tbreak\n",
        "\t\tif row[0] == \"*vertices\":\n",
        "\t\t\tprint(\"GOT vertices\")\n",
        "\t\t\tcontinue\n",
        "\t\tpajekGraphID = int(row[0])\n",
        "\t\tnetworkXGraphID = int(row[1])\n",
        "\t\tprint(pajekGraphID,networkXGraphID)\n",
        "\t\tif pajekGraphID not in PajekGraphID2NetworkxGraphID:\n",
        "\t\t\tPajekGraphID2NetworkxGraphID[pajekGraphID] = networkXGraphID\n",
        "\t\telse:\n",
        "\t\t\ttry:\n",
        "\t\t\t\traise KeyboardInterrupt\n",
        "\t\t\tfinally:\n",
        "\t\t\t\tprint('Self-Defined Error:Duplicate Value Exception in makePajekGraphIDtoNetworkXID Function')\n",
        "\treturn PajekGraphID2NetworkxGraphID\n",
        "\n",
        "def get_Coreness(corefilename, PajekGraphID2NetworkxGraphID):\n",
        "    rows = openCSVfile(corefilename,\"\t\")\n",
        "    rows = rows[1:]\n",
        "    coreNessDictionary = {}\n",
        "    for row_no,row in enumerate(rows):\n",
        "        print(row_no,row)\n",
        "        graphID = PajekGraphID2NetworkxGraphID[row_no + 1]\n",
        "        WeightedCoreness = int(row[0])\n",
        "        coreNessDictionary[graphID] = WeightedCoreness\n",
        "    return coreNessDictionary\n",
        "\n",
        "#------------------------------- K_CORE ---------------------\n",
        "def makeKcore(graph,coreNessDictionary):\n",
        "\tdegNodes = []\n",
        "\tperipheryNodes = []\n",
        "\tmax_coreness = -1\n",
        "\n",
        "\tfor vertex in coreNessDictionary:\n",
        "\t\tw_coreness = coreNessDictionary[vertex]\n",
        "\t\tif max_coreness < w_coreness:\n",
        "\t\t\tmax_coreness = w_coreness\n",
        "\n",
        "\tprint(\"Maximum Weighted Coreness = \",max_coreness)\n",
        "\tprint(\"Found \")\n",
        "\n",
        "\tfor vertex in coreNessDictionary:\n",
        "\t\tweightedCoreness = coreNessDictionary[vertex]\n",
        "\t\tif weightedCoreness >= max_coreness:\n",
        "\t\t\tdegNodes.append(str(vertex))\n",
        "\t\telse:\n",
        "\t\t\tperipheryNodes.append(str(vertex))\n",
        "\n",
        "\tprint(\"Making the Degerency Core....\")\n",
        "\tdegCore = graph.subgraph(degNodes)\n",
        "\n",
        "\tprint(\"Making the Periphery Graph\")\n",
        "\tperipheryGraph = graph.subgraph(peripheryNodes)\n",
        "\treturn degCore,peripheryGraph\n",
        "\n",
        "#------------------------------- Pajek as K_CORE ---------------------\n",
        "def makeKcore(graph,coreNessDictionary):\n",
        "    degNodes = []\n",
        "    peripheryNodes = []\n",
        "    max_coreness = -1\n",
        "    \n",
        "    for vertex in coreNessDictionary:\n",
        "        w_coreness = coreNessDictionary[vertex]\n",
        "        if max_coreness < w_coreness:\n",
        "            max_coreness = w_coreness\n",
        "    \n",
        "    print(\"Maximum Weighted Coreness = \",max_coreness)\n",
        "    print(\"Found \")\n",
        "    \n",
        "    for vertex in coreNessDictionary:\n",
        "        weightedCoreness = coreNessDictionary[vertex]\n",
        "        if weightedCoreness >= max_coreness:\n",
        "            degNodes.append(str(vertex))\n",
        "        else:\n",
        "            peripheryNodes.append(str(vertex))\n",
        "    \n",
        "    print(\"Making the Degerency Core....\")\n",
        "    degCore = graph.subgraph(degNodes)\n",
        "    \n",
        "    print(\"Making the Periphery Graph\")\n",
        "    peripheryGraph = graph.subgraph(peripheryNodes)\n",
        "    return degCore,peripheryGraph\n",
        "\n",
        "\n",
        "\n",
        "#------------------------Centrality Functions--------------------------------------------------\n",
        "def makeEigenVectorCentralityDict(graph,weight = False):\n",
        "    eigenvector_cent= {}\n",
        "    if weight == False:\n",
        "        eigenvector_cent = nx.eigenvector_centrality(graph, max_iter = 100, tol = 1e-06, nstart = None,weight = None)\n",
        "    else:\n",
        "        eigenvector_cent = nx.eigenvector_centrality(graph, max_iter = 100, tol = 1e-06, nstart = None, weight = 'weight')\n",
        "    return eigenvector_cent\n",
        "    \n",
        "def makePageRank(graph,weight = False):\n",
        "    page_rank = []\n",
        "    if weight == False:\n",
        "        page_rank = nx.pagerank(graph, alpha = 0.85, personalization = None, max_iter = 100, tol = 1e-06, nstart = None, weight = None, dangling = None)\n",
        "    else:\n",
        "        page_rank = nx.pagerank(G, alpha = 0.85, personalization = None, max_iter = 100, tol=1e-06, nstart = None, weight = 'weight', dangling = None)\n",
        "    return page_rank\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------Make Degrees as a Dictionary-------------------------------------\n",
        "\n",
        "def makeWeightedDegreeDictionary(graph):\n",
        "    weighted_vertices = sorted(graph.degree(weight = \"weight\"), key = lambda x: x[1], reverse = True)\n",
        "    WeightedDegreeDictionary = {}\n",
        "    for vertex in weighted_vertices:\n",
        "        WeightedDegreeDictionary[vertex[0]] = vertex[1]\n",
        "    return WeightedDegreeDictionary\n",
        "    \n",
        "def makeDegreeDictionary(graph):\n",
        "    vertices = sorted(graph.degree, key = lambda x: x[1], reverse = True)\n",
        "    DegreeDictionary = {}\n",
        "    for vertex in vertices:\n",
        "        DegreeDictionary[vertex[0]] = vertex[1]\n",
        "    return DegreeDictionary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------Normalize Dictionary------------------------------------\n",
        "\n",
        "def normalizeDict(dictionary):\n",
        "    maxValue = -1\n",
        "    new_dictionary = {}\n",
        "    for v in dictionary:\n",
        "        value = dictionary[v]\n",
        "        if value > maxValue:\n",
        "            maxValue = value\n",
        "    for v in dictionary:\n",
        "        new_value = dictionary[v]/maxValue\n",
        "        new_dictionary[v] = new_value\n",
        "    return new_dictionary\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def makeRandomWeightedGraph(n = 10,m = 10,weight_range = 10):\n",
        "\tG = nx.gnm_random_graph(n,m)\n",
        "\tfor (u,v,w) in G.edges(data=True):\n",
        "\t\tw['weight'] = random.randint(0,weight_range)\n",
        "\treturn G\n",
        "\n",
        "def getAdjacencyMatrix(G,weighted  = True,format = \"numpy\"):\n",
        "\tadj = {}\n",
        "\tif weighted == True:\n",
        "\t\tadj = nx.adjacency_matrix(G, weight='weight')\n",
        "\telse:\n",
        "\t\tadj = nx.adjacency_matrix(G)\n",
        "\tif format == \"numpy\":\n",
        "\t\treturn xn.to_numpy_matrix(adj)\n",
        "\telif format == \"dict\":\n",
        "\t\treturn to_dict_of_dicts(adj)\n",
        "\telif format == \"scipy\":\n",
        "\t\treturn to_scipy_sparse_matrix(adj)\n",
        "\telse:\n",
        "\t\tprint(\"Invalid Format , returning in numpy format\")\n",
        "\t\treturn to_numpy_matrix(adj)\n"
      ],
      "metadata": {
        "id": "Uckcbb5UXgxF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "# import basic_functions\n",
        "# import graph_functions\n",
        "import keras\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from numpy.random import seed\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense, Conv1D,Conv2D,Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers.merge import concatenate\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recallA = true_positives / (possible_positives + K.epsilon())\n",
        "    return recallA\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precisionA = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precisionA\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    precisionA = precision(y_true, y_pred)\n",
        "    recallA = recall(y_true, y_pred)\n",
        "    return 2*((precisionA*recallA)/(precisionA+recallA+K.epsilon()))\n",
        "\n",
        "\n",
        "\n",
        "base_folder = \"drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data/\"\n",
        "# working_folder_path = base_folder + \"New_Data/Min_Graph/\"\n",
        "# Author2GraphIDPath = working_folder_path + \"Author2GraphID.json\"\n",
        "# metadata_embeddingsPath = working_folder_path + \"metadata_embeddingsPath\"\n",
        "# final_embeddingsPath = working_folder_path + \"final_video_textual_embeddings.json\"\n",
        "# core_usersPath = working_folder_path + \"core_users.pkl\"\n",
        "# non_core_usersPath = working_folder_path + \"non_core_users.pkl\"\n",
        "# userEmbeddingsPath = working_folder_path + \"userEmbeddingsPath.pkl\"\n",
        "# modelpath = working_folder_path + \"basic_model_1.pkl\"\n",
        "# video_cosine_similarityPath = working_folder_path + \"video_cosine_similarity.pkl\"\n",
        "# comment_self_comment_cosine_similarity_Path = working_folder_path + \"comment_self_comment_cosine_similarity.pkl\"\n",
        "# commentSimFeaturesPath = working_folder_path + \"commentSimilarityFeatures.pkl\"\n",
        "# videoSimFeaturesPath = working_folder_path + \"videoSimilarityFeatures.pkl\"\n",
        "#-------------------------------------------------------------\n",
        "models_path = base_folder + \"/Models/\"\n",
        "#------------------------------------------------------------\n",
        "Prem_model1path = models_path + \"Prem_model1.pkl\"\n",
        "Prem_model2path = models_path + \"Prem_model2.pkl\"\n",
        "Prem_model3path = models_path + \"Prem_model3.pkl\"\n",
        "Prem_model4path = models_path + \"Prem_model4.pkl\"\n",
        "Prem_model5path = models_path + \"Prem_model5.pkl\"\n",
        "Prem_model6path = models_path + \"Prem_model6.pkl\"\n",
        "Prem_model7path = models_path + \"Prem_model7.pkl\"\n",
        "Prem_model8path = models_path + \"Prem_model8.pkl\"\n",
        "Prem_model9path = models_path + \"Prem_model9.pkl\"\n",
        "Prem_model10path = models_path + \"Prem_model10.pkl\"\n",
        "Prem_model11path = models_path + \"Prem_model11.pkl\"\n",
        "Prem_model12path = models_path + \"Prem_model12.pkl\"\n",
        "Prem_model13path = models_path + \"Prem_model13.pkl\"\n",
        "Prem_model14path = models_path + \"Prem_model14.pkl\"\n",
        "Prem_model15path = models_path + \"Prem_model15.pkl\"\n",
        "#--------------------------------------------------------------\n",
        "model1path = models_path + \"model1.h5\"\n",
        "model2path = models_path + \"model2.h5\"\n",
        "model3path = models_path + \"model3.h5\"\n",
        "model4path = models_path + \"model4.h5\"\n",
        "model5path = models_path + \"model5.h5\"\n",
        "model6path = models_path + \"model6.h5\"\n",
        "model7path = models_path + \"model7.h5\"\n",
        "model8path = models_path + \"model8.h5\"\n",
        "model9path = models_path + \"model9.h5\"\n",
        "model10path = models_path + \"model10.h5\"\n",
        "model11path = models_path + \"model11.h5\"\n",
        "model12path = models_path + \"model12.h5\"\n",
        "model13path = models_path + \"model13.h5\"\n",
        "model14path = models_path + \"model14.h5\"\n",
        "model15path = models_path + \"model15.h5\"\n",
        "#---------------------------------------------------------------------\n",
        "Dataset1 = base_folder + \"/Model_Data/1_to_1/\"\n",
        "Dataset2 = base_folder + \"/Model_Data/1_to_2/\"\n",
        "Dataset3 = base_folder + \"/Model_Data/1_to_3/\"\n",
        "Dataset4 = base_folder + \"/Model_Data/1_to_4/\"\n",
        "\n",
        "\n",
        "result_dictionaries = []\n",
        "DatasetPath = Dataset1\n",
        "premlim_path = Prem_model11path\n",
        "best_model_path = model11path\n",
        "modelResultsFile = \"model11_results.pkl\"\n",
        "modelAveragedResultsJson = \"model11_avg_results.json\"\n",
        "#-------------------------------------------------------------------------------------\n",
        "nfmeta_features = 31\n",
        "nfcomment_features = 768\n",
        "nfsim_features = 25\n",
        "\n",
        "def model8(init_mode = 'he_uniform', activation = 'relu', nfhidden_neurons = 128,dropout_rate = 0.75):\n",
        "    num_filters = 32;filter_size = 2;\n",
        "\n",
        "    input2 = Input(shape = (nfmeta_features,), name = 'meta_input1')\n",
        "    y = Dense(32,input_dim = nfmeta_features,activation = 'relu',name = 'meta_branch1')(input2)\n",
        "    y = keras.layers.Dropout(0.2)(y)\n",
        "\n",
        "    \n",
        "    input3 = Input(shape = (nfsim_features,), name = 'sim_input1')\n",
        "    y1 = Dense(32,input_dim = nfmeta_features,activation = 'relu',name = 'sim_branch1')(input3)\n",
        "    y1 = keras.layers.Dropout(0.2)(y1)\n",
        "    \n",
        "    input1 = Input(shape = (nfcomment_features,1), name = 'text_input')\n",
        "\n",
        "    x = Conv1D(num_filters, filter_size, input_shape = (None,nfcomment_features,1))(input1)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = keras.layers.MaxPooling1D(pool_size = 2, strides=None, padding='valid', data_format='channels_last')(x)\n",
        "\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(8, activation='relu')(x)#Low\n",
        "    x = keras.layers.Dropout(0.2)(x)#High\n",
        "\n",
        "\n",
        "    conc = concatenate([y1,x,y])\n",
        "\n",
        "\n",
        "    conc = Dense(8, activation = 'relu')(conc)\n",
        "    conc = keras.layers.Dropout(0.2)(conc)\n",
        "\n",
        "    predictions = Dense(2, activation='softmax')(conc)\n",
        "    model = Model([input1,input2,input3], outputs = predictions)\n",
        "    #model = Model([input1,input3], outputs = predictions)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "def getDataset(DatasetPath,nfmeta_features,nfcomment_features,nfsim_features):\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    print(DatasetPath)\n",
        "    print(os.path.exists(DatasetPath))\n",
        "    print(os.path.exists(DatasetPath + \"splits_final.pkl\"))\n",
        "    splits = loadPickleFile(DatasetPath + \"splits_final.pkl\")\n",
        "    comment_features_test = splits[\"comment_features_test\"]\n",
        "    meta_features_test = splits[\"meta_features_test\"]\n",
        "    sim_features_test = splits[\"sim_features_test\"]\n",
        "    \n",
        "    users_tr = splits[\"users_tr\"]\n",
        "    users_test = splits[\"users_test\"]\n",
        "    tr_targets = splits[\"targets_tr\"] \n",
        "    test_targets = splits[\"targets_test\"]\n",
        "    \n",
        "    print(comment_features_test.shape,meta_features_test.shape,len(test_targets))\n",
        "    \n",
        "    comment_features_train = splits[\"split_no\" + str(split_no)][\"comment_features_train\"]\n",
        "    meta_features_train = splits[\"split_no\" + str(split_no)][\"meta_features_train\"]\n",
        "    sim_features_train = splits[\"split_no\" + str(split_no)][\"sim_features_train\"]\n",
        "    comment_features_val = splits[\"split_no\" + str(split_no)][\"comment_features_val\"]\n",
        "    meta_features_val = splits[\"split_no\" + str(split_no)][\"meta_features_val\"]\n",
        "    sim_features_val = splits[\"split_no\" + str(split_no)][\"sim_features_val\"]\n",
        "    train_targets = splits[\"split_no\" + str(split_no)][\"targets_train\"]\n",
        "    val_targets = splits[\"split_no\" + str(split_no)][\"targets_val\"]\n",
        "    users_train = list(splits[\"split_no\" + str(split_no)][\"users_train\"])\n",
        "    users_val = list(splits[\"split_no\" + str(split_no)][\"users_val\"])\n",
        "\n",
        "    print(comment_features_train.shape,meta_features_train.shape,len(train_targets))\n",
        "    \n",
        "    train_targets = enc.fit_transform(np.array(train_targets).reshape(-1,1))\n",
        "    val_targets = enc.transform(np.array(val_targets).reshape(-1,1))\n",
        "    tr_targets = enc.transform(np.array(tr_targets).reshape(-1,1))\n",
        "    test_targets = enc.transform(np.array(test_targets).reshape(-1,1))\n",
        "    \n",
        "    comment_features_train = normalize(comment_features_train, axis = 0)\n",
        "    comment_features_val = normalize(comment_features_val, axis = 0)\n",
        "    comment_features_test = normalize(comment_features_test,axis = 0)\n",
        "    \n",
        "    sim_features_train = normalize(sim_features_train, axis = 0)[:,:nfsim_features]\n",
        "    sim_features_val = normalize(sim_features_val, axis = 0)[:,:nfsim_features]\n",
        "    sim_features_test = normalize(sim_features_test,axis = 0)[:,:nfsim_features]\n",
        "    \n",
        "    meta_feature_train2 = normalize(meta_features_train,axis = 0)\n",
        "    meta_feature_test2 = normalize(meta_features_test,axis = 0)\n",
        "    meta_feature_val2 = normalize(meta_features_val,axis = 0)\n",
        "\n",
        "    meta_feature_train = meta_feature_train2[:,:nfmeta_features]\n",
        "    meta_feature_test = meta_feature_test2[:,:nfmeta_features]\n",
        "    meta_feature_val = meta_feature_val2[:,:nfmeta_features]\n",
        "    \n",
        "    return comment_features_train,comment_features_val,comment_features_test,meta_feature_train,meta_feature_val,meta_feature_test,sim_features_train,sim_features_val,sim_features_test,users_train,users_val,users_test,train_targets,val_targets,test_targets\n",
        "\n",
        "\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "\n",
        "for split_no in range(1,11):\n",
        "    print(\"-----------Split_Number---------\",split_no)\n",
        "    comment_features_train,comment_features_val,comment_features_test,meta_feature_train,meta_feature_val,meta_feature_test,sim_features_train,sim_features_val,sim_features_test,users_train,users_val,users_test,train_targets,val_targets,test_targets = getDataset(DatasetPath,nfmeta_features,nfcomment_features,nfsim_features)\n",
        "    nfepochs = 50;lr = 0.01;bs = 32;b1 = 0.9;b2 = 0.999\n",
        "    \n",
        "    comment_features_train_3d = np.expand_dims(comment_features_train, axis = 2)\n",
        "    comment_features_val_3d = np.expand_dims(comment_features_val, axis = 2)\n",
        "    comment_features_test_3d = np.expand_dims(comment_features_test, axis = 2)\n",
        "\n",
        "    adamopt = keras.optimizers.Adam(lr = 0.002 , beta_1 = 0.9, beta_2 = 0.999)\n",
        "    modelcheck = keras.callbacks.ModelCheckpoint(premlim_path, monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'auto', period=1)#Callback\n",
        "    new_model = model8()\n",
        "    #sim_features_train,sim_features_val,sim_features_test\n",
        "    new_model.compile(optimizer = adamopt , loss = 'categorical_crossentropy', metrics=[precision])# Compile Model\n",
        "    history = new_model.fit([comment_features_train_3d,meta_feature_train,sim_features_train], train_targets,batch_size = bs,validation_data = [[comment_features_val_3d,meta_feature_val,sim_features_val],val_targets], epochs = nfepochs, verbose = 1,callbacks = [modelcheck])#Fit Model\n",
        "    #history = new_model.fit([comment_features_train_3d,sim_features_train], train_targets,batch_size = bs,validation_data = [[comment_features_val_3d,sim_features_val],val_targets], epochs = nfepochs, verbose = 1,callbacks = [modelcheck])#Fit Model\n",
        "    \n",
        "    del new_model\n",
        "    \n",
        "    model_to_load = model8()\n",
        "    model_to_load.load_weights(premlim_path)\n",
        "\n",
        "    #yp = model.predict([meta_feature_test,comment_features_test], verbose = 1)\n",
        "    #yp = model.predict([meta], verbose = 1)\n",
        "    #avg_yp.append(yp)\n",
        "    yp = model_to_load.predict([comment_features_test_3d,meta_feature_test,sim_features_test], verbose = 1)\n",
        "    #yp = model_to_load.predict([comment_features_test_3d,sim_features_test], verbose = 1)\n",
        "    y_true = np.argmax(test_targets,axis = 1)\n",
        "    y_pred = np.argmax(yp,axis = 1)\n",
        "    result_dict = classification_report(y_true, y_pred,digits = 4,output_dict=True)\n",
        "    \n",
        "    print(result_dict)\n",
        "    result_dictionaries.append(result_dict)\n",
        "    print(len(result_dictionaries)) \n",
        "\n",
        "import copy\n",
        "def calc_average(result_dictionaries):\n",
        "    \n",
        "    p0s = [];r0s = [];f0s = []\n",
        "    p1s = [];r1s = [];f1s = []\n",
        "    mp = [];mr = [];mf = []\n",
        "    wap = [];war = [];waf1 = []\n",
        "    accs = []\n",
        "    \n",
        "    final_dict = copy.deepcopy(result_dictionaries[0])\n",
        "    \n",
        "    for i,dicti in enumerate(result_dictionaries):\n",
        "        \n",
        "        print(\"---------------\",i,\"-----------------\")\n",
        "        print(dicti)\n",
        "        \n",
        "        p0 = dicti['0']['precision']\n",
        "        r0 = dicti['0']['recall']\n",
        "        f0 = dicti['0']['f1-score']\n",
        "        p0s.append(p0);r0s.append(r0);f0s.append(f0)\n",
        "        \n",
        "        p1 = dicti['1']['precision']\n",
        "        r1 = dicti['1']['recall']\n",
        "        f1 = dicti['1']['f1-score']\n",
        "        p1s.append(p1);r1s.append(r1);f1s.append(f1)\n",
        "        \n",
        "        acc = dicti['accuracy']\n",
        "        accs.append(acc)\n",
        "        \n",
        "        m1 = dicti['macro avg']['precision']\n",
        "        m2 = dicti['macro avg']['recall'] \n",
        "        m3 = dicti['macro avg']['f1-score']\n",
        "        mp.append(m1);mr.append(m2);mf.append(m3)\n",
        "        \n",
        "        w1 = dicti['weighted avg']['precision']\n",
        "        w2 = dicti['weighted avg']['recall'] \n",
        "        w3 = dicti['weighted avg']['f1-score']\n",
        "        wap.append(w1);war.append(w2);waf1.append(w3)\n",
        "        \n",
        "        \n",
        "    final_dict['0']['precision'] = np.mean(p0s)\n",
        "    final_dict['0']['recall'] = np.mean(r0s)\n",
        "    final_dict['0']['f1-score'] = np.mean(f0s)\n",
        "    final_dict['1']['precision'] = np.mean(p1s)\n",
        "    final_dict['1']['recall'] = np.mean(r1s)\n",
        "    final_dict['1']['f1-score'] = np.mean(f1s)\n",
        "    final_dict['macro avg']['precision'] = np.mean(mp)\n",
        "    final_dict['macro avg']['recall'] = np.mean(mr)\n",
        "    final_dict['macro avg']['f1-score'] = np.mean(mf)\n",
        "    final_dict['weighted avg']['precision'] = np.mean(wap)\n",
        "    final_dict['weighted avg']['recall'] = np.mean(war)\n",
        "    final_dict['weighted avg']['f1-score'] = np.mean(waf1)\n",
        "    \n",
        "    \n",
        "    ans = [np.mean(p0s), np.mean(r0s), np.mean(f0s), np.mean(p1s), np.mean(r1s), np.mean(f1s), np.mean(mp), np.mean(mr), np.mean(mf)]\n",
        "    ans.append(np.mean(wap)) \n",
        "    ans.append(np.mean(war)) \n",
        "    ans.append(np.mean(waf1))\n",
        "    ans.append(np.mean(accs))\n",
        "    \n",
        "    return final_dict\n",
        "\n",
        "\n",
        "averaged_out_results = calc_average(result_dictionaries)\n",
        "print('FINAL')\n",
        "print(averaged_out_results)\n",
        "\n",
        "p = averaged_out_results['macro avg']['precision']\n",
        "r = averaged_out_results['macro avg']['recall']\n",
        "f = 2*(p*r)/(p+r)\n",
        "\n",
        "print(p,r,f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmundUOuY0CG",
        "outputId": "b46faaca-76e2-40d4-f9bc-6d15a5d2021a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------Split_Number--------- 1\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "159/159 [==============================] - 1s 5ms/step - loss: 0.7391 - precision: 0.5346 - val_loss: 0.5475 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54753, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 883us/step - loss: 0.6130 - precision: 0.6289 - val_loss: 0.5326 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54753 to 0.53262, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 806us/step - loss: 0.5564 - precision: 0.7170 - val_loss: 0.4745 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.53262 to 0.47453, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 851us/step - loss: 0.5180 - precision: 0.7610 - val_loss: 0.4012 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.47453 to 0.40116, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 821us/step - loss: 0.5524 - precision: 0.7736 - val_loss: 0.3327 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.40116 to 0.33268, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 812us/step - loss: 0.5197 - precision: 0.7799 - val_loss: 0.3034 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.33268 to 0.30342, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 871us/step - loss: 0.4505 - precision: 0.8113 - val_loss: 0.2959 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.30342 to 0.29593, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 791us/step - loss: 0.4587 - precision: 0.7925 - val_loss: 0.2901 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.29593 to 0.29015, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 826us/step - loss: 0.3773 - precision: 0.8491 - val_loss: 0.2922 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.29015\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 808us/step - loss: 0.3502 - precision: 0.8428 - val_loss: 0.3593 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.29015\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 867us/step - loss: 0.3228 - precision: 0.8742 - val_loss: 0.3665 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.29015\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 791us/step - loss: 0.3289 - precision: 0.8428 - val_loss: 0.4064 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.29015\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 843us/step - loss: 0.2821 - precision: 0.8742 - val_loss: 0.4351 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.29015\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 859us/step - loss: 0.3220 - precision: 0.8428 - val_loss: 0.3591 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.29015\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 859us/step - loss: 0.2421 - precision: 0.9119 - val_loss: 0.4879 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.29015\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 818us/step - loss: 0.2509 - precision: 0.8679 - val_loss: 0.4437 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.29015\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 829us/step - loss: 0.2465 - precision: 0.8868 - val_loss: 0.4307 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.29015\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 843us/step - loss: 0.2392 - precision: 0.8868 - val_loss: 0.5577 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.29015\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 830us/step - loss: 0.2367 - precision: 0.8931 - val_loss: 0.5911 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.29015\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 829us/step - loss: 0.2403 - precision: 0.8994 - val_loss: 0.6400 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.29015\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 858us/step - loss: 0.2206 - precision: 0.9119 - val_loss: 0.6496 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.29015\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 823us/step - loss: 0.2076 - precision: 0.9371 - val_loss: 0.6875 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.29015\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 888us/step - loss: 0.1945 - precision: 0.9308 - val_loss: 0.7485 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.29015\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 827us/step - loss: 0.1915 - precision: 0.9245 - val_loss: 1.0463 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.29015\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 817us/step - loss: 0.2232 - precision: 0.8805 - val_loss: 0.8601 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.29015\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 824us/step - loss: 0.1533 - precision: 0.9497 - val_loss: 0.6928 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.29015\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 812us/step - loss: 0.1977 - precision: 0.8931 - val_loss: 0.8404 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.29015\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 868us/step - loss: 0.1805 - precision: 0.9245 - val_loss: 0.9642 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.29015\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 860us/step - loss: 0.1535 - precision: 0.9497 - val_loss: 0.7921 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.29015\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 829us/step - loss: 0.1348 - precision: 0.9874 - val_loss: 0.9775 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.29015\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 825us/step - loss: 0.1559 - precision: 0.9497 - val_loss: 1.1886 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.29015\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 856us/step - loss: 0.1637 - precision: 0.9245 - val_loss: 1.1745 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.29015\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 822us/step - loss: 0.1360 - precision: 0.9497 - val_loss: 0.8758 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.29015\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 821us/step - loss: 0.1528 - precision: 0.9371 - val_loss: 1.0303 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.29015\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 840us/step - loss: 0.1196 - precision: 0.9497 - val_loss: 1.4159 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.29015\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 910us/step - loss: 0.1457 - precision: 0.9308 - val_loss: 1.4604 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.29015\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 824us/step - loss: 0.1086 - precision: 0.9748 - val_loss: 1.1849 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.29015\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 836us/step - loss: 0.1206 - precision: 0.9497 - val_loss: 1.2730 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.29015\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 831us/step - loss: 0.1098 - precision: 0.9748 - val_loss: 1.1621 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.29015\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 836us/step - loss: 0.1063 - precision: 0.9434 - val_loss: 1.0462 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.29015\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 914us/step - loss: 0.0858 - precision: 0.9748 - val_loss: 1.2975 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.29015\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 886us/step - loss: 0.0911 - precision: 0.9748 - val_loss: 1.5119 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.29015\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 894us/step - loss: 0.0949 - precision: 0.9623 - val_loss: 1.3848 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.29015\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 832us/step - loss: 0.0860 - precision: 0.9686 - val_loss: 1.4800 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.29015\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 872us/step - loss: 0.1054 - precision: 0.9623 - val_loss: 1.5538 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.29015\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 913us/step - loss: 0.0794 - precision: 0.9811 - val_loss: 1.6481 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.29015\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 824us/step - loss: 0.0745 - precision: 0.9748 - val_loss: 1.6333 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.29015\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 848us/step - loss: 0.0807 - precision: 1.0000 - val_loss: 1.6023 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.29015\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 853us/step - loss: 0.0749 - precision: 0.9811 - val_loss: 1.6485 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.29015\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 853us/step - loss: 0.0587 - precision: 0.9874 - val_loss: 1.7064 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.29015\n",
            "119/119 [==============================] - 0s 1ms/step\n",
            "{'0': {'precision': 0.8, 'recall': 0.6779661016949152, 'f1-score': 0.7339449541284404, 'support': 59}, '1': {'precision': 0.7246376811594203, 'recall': 0.8333333333333334, 'f1-score': 0.7751937984496124, 'support': 60}, 'accuracy': 0.7563025210084033, 'macro avg': {'precision': 0.7623188405797101, 'recall': 0.7556497175141244, 'f1-score': 0.7545693762890264, 'support': 119}, 'weighted avg': {'precision': 0.7620021921812203, 'recall': 0.7563025210084033, 'f1-score': 0.754742690760964, 'support': 119}}\n",
            "1\n",
            "-----------Split_Number--------- 2\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "159/159 [==============================] - 1s 6ms/step - loss: 0.6895 - precision: 0.5186 - val_loss: 0.7406 - val_precision: 0.4444\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.74062, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 878us/step - loss: 0.6737 - precision: 0.5723 - val_loss: 0.7321 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.74062 to 0.73206, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 793us/step - loss: 0.6095 - precision: 0.6541 - val_loss: 0.6220 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.73206 to 0.62203, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 854us/step - loss: 0.5800 - precision: 0.6855 - val_loss: 0.6505 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.62203\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 842us/step - loss: 0.5245 - precision: 0.7987 - val_loss: 0.8741 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.62203\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 849us/step - loss: 0.4410 - precision: 0.8239 - val_loss: 0.9277 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.62203\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 875us/step - loss: 0.5241 - precision: 0.7925 - val_loss: 0.8913 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.62203\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 858us/step - loss: 0.4409 - precision: 0.8176 - val_loss: 0.8561 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.62203\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 875us/step - loss: 0.3553 - precision: 0.8805 - val_loss: 1.1072 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.62203\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 837us/step - loss: 0.3281 - precision: 0.8679 - val_loss: 1.1842 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.62203\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 887us/step - loss: 0.3164 - precision: 0.8931 - val_loss: 1.4231 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.62203\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 900us/step - loss: 0.3014 - precision: 0.8805 - val_loss: 1.6252 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.62203\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 831us/step - loss: 0.2674 - precision: 0.9057 - val_loss: 1.3355 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.62203\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 845us/step - loss: 0.2566 - precision: 0.9057 - val_loss: 0.9155 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.62203\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 826us/step - loss: 0.2550 - precision: 0.9057 - val_loss: 0.9903 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.62203\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 879us/step - loss: 0.2983 - precision: 0.8616 - val_loss: 0.7033 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.62203\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 834us/step - loss: 0.2083 - precision: 0.9308 - val_loss: 0.7823 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.62203\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 837us/step - loss: 0.2175 - precision: 0.9182 - val_loss: 0.7109 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.62203\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 887us/step - loss: 0.2281 - precision: 0.9308 - val_loss: 1.0649 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.62203\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 829us/step - loss: 0.1635 - precision: 0.9560 - val_loss: 1.1193 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.62203\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 840us/step - loss: 0.1735 - precision: 0.9308 - val_loss: 1.2681 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.62203\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 788us/step - loss: 0.1760 - precision: 0.9245 - val_loss: 2.0489 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.62203\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 851us/step - loss: 0.1696 - precision: 0.9182 - val_loss: 1.4862 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.62203\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 885us/step - loss: 0.1661 - precision: 0.9371 - val_loss: 1.5225 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.62203\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 837us/step - loss: 0.1273 - precision: 0.9497 - val_loss: 2.0093 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.62203\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 889us/step - loss: 0.1313 - precision: 0.9560 - val_loss: 2.0245 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.62203\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 886us/step - loss: 0.1437 - precision: 0.9434 - val_loss: 1.4595 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.62203\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 813us/step - loss: 0.1242 - precision: 0.9560 - val_loss: 1.9578 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.62203\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 842us/step - loss: 0.1252 - precision: 0.9560 - val_loss: 1.9804 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.62203\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 811us/step - loss: 0.1348 - precision: 0.9308 - val_loss: 1.3902 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.62203\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 878us/step - loss: 0.1048 - precision: 0.9686 - val_loss: 1.9363 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.62203\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 802us/step - loss: 0.1162 - precision: 0.9560 - val_loss: 2.8475 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.62203\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 852us/step - loss: 0.0934 - precision: 0.9623 - val_loss: 2.6507 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.62203\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 944us/step - loss: 0.0865 - precision: 0.9748 - val_loss: 2.8377 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.62203\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 834us/step - loss: 0.1006 - precision: 0.9748 - val_loss: 2.7048 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.62203\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 845us/step - loss: 0.0726 - precision: 0.9748 - val_loss: 2.7780 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.62203\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 861us/step - loss: 0.0831 - precision: 0.9874 - val_loss: 3.6651 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.62203\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 870us/step - loss: 0.0742 - precision: 0.9748 - val_loss: 2.2093 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.62203\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 905us/step - loss: 0.0956 - precision: 0.9623 - val_loss: 1.6184 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.62203\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 825us/step - loss: 0.0948 - precision: 0.9686 - val_loss: 1.4381 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.62203\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 805us/step - loss: 0.0808 - precision: 0.9748 - val_loss: 1.6965 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.62203\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 844us/step - loss: 0.0812 - precision: 0.9686 - val_loss: 1.7958 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.62203\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 810us/step - loss: 0.0722 - precision: 0.9874 - val_loss: 1.4415 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.62203\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 836us/step - loss: 0.0606 - precision: 0.9874 - val_loss: 2.1949 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.62203\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 850us/step - loss: 0.0758 - precision: 0.9686 - val_loss: 1.4061 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.62203\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 922us/step - loss: 0.0867 - precision: 0.9686 - val_loss: 1.0560 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.62203\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 933us/step - loss: 0.1076 - precision: 0.9623 - val_loss: 1.8425 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.62203\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 867us/step - loss: 0.0893 - precision: 0.9748 - val_loss: 1.8354 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.62203\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 864us/step - loss: 0.0657 - precision: 0.9748 - val_loss: 1.5505 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.62203\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 875us/step - loss: 0.0984 - precision: 0.9686 - val_loss: 1.3139 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.62203\n",
            "119/119 [==============================] - 0s 2ms/step\n",
            "{'0': {'precision': 0.7543859649122807, 'recall': 0.7288135593220338, 'f1-score': 0.7413793103448276, 'support': 59}, '1': {'precision': 0.7419354838709677, 'recall': 0.7666666666666667, 'f1-score': 0.7540983606557377, 'support': 60}, 'accuracy': 0.7478991596638656, 'macro avg': {'precision': 0.7481607243916242, 'recall': 0.7477401129943503, 'f1-score': 0.7477388355002826, 'support': 119}, 'weighted avg': {'precision': 0.7481084114460724, 'recall': 0.7478991596638656, 'f1-score': 0.7477922768881436, 'support': 119}}\n",
            "2\n",
            "-----------Split_Number--------- 3\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "159/159 [==============================] - 1s 8ms/step - loss: 0.7285 - precision: 0.5498 - val_loss: 0.6460 - val_precision: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.64601, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 828us/step - loss: 0.6742 - precision: 0.6478 - val_loss: 0.6276 - val_precision: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.64601 to 0.62756, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 867us/step - loss: 0.6621 - precision: 0.6226 - val_loss: 0.6102 - val_precision: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.62756 to 0.61019, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 888us/step - loss: 0.6383 - precision: 0.7484 - val_loss: 0.5865 - val_precision: 0.5000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.61019 to 0.58654, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 841us/step - loss: 0.6314 - precision: 0.7170 - val_loss: 0.5634 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.58654 to 0.56341, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 853us/step - loss: 0.6142 - precision: 0.8176 - val_loss: 0.5383 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.56341 to 0.53826, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 912us/step - loss: 0.5878 - precision: 0.8176 - val_loss: 0.5187 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.53826 to 0.51867, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 896us/step - loss: 0.5717 - precision: 0.8553 - val_loss: 0.5001 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.51867 to 0.50009, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 860us/step - loss: 0.5453 - precision: 0.8868 - val_loss: 0.4747 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.50009 to 0.47468, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 889us/step - loss: 0.5414 - precision: 0.8428 - val_loss: 0.4514 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.47468 to 0.45138, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 836us/step - loss: 0.5059 - precision: 0.9057 - val_loss: 0.4233 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.45138 to 0.42326, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 831us/step - loss: 0.4805 - precision: 0.8994 - val_loss: 0.4027 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.42326 to 0.40265, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 926us/step - loss: 0.4682 - precision: 0.8679 - val_loss: 0.3804 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.40265 to 0.38036, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 889us/step - loss: 0.4312 - precision: 0.9182 - val_loss: 0.3428 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.38036 to 0.34281, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 870us/step - loss: 0.4426 - precision: 0.8805 - val_loss: 0.3202 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.34281 to 0.32017, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 881us/step - loss: 0.4158 - precision: 0.8805 - val_loss: 0.2971 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.32017 to 0.29708, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 829us/step - loss: 0.3788 - precision: 0.8994 - val_loss: 0.2838 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.29708 to 0.28382, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 947us/step - loss: 0.3559 - precision: 0.9245 - val_loss: 0.2537 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.28382 to 0.25366, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 854us/step - loss: 0.3519 - precision: 0.8931 - val_loss: 0.2365 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.25366 to 0.23654, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 870us/step - loss: 0.3549 - precision: 0.8805 - val_loss: 0.2186 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.23654 to 0.21859, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 868us/step - loss: 0.3130 - precision: 0.9119 - val_loss: 0.2026 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.21859 to 0.20261, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 898us/step - loss: 0.3112 - precision: 0.9182 - val_loss: 0.1934 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.20261 to 0.19340, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 950us/step - loss: 0.2990 - precision: 0.9057 - val_loss: 0.1767 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.19340 to 0.17672, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 880us/step - loss: 0.2872 - precision: 0.9119 - val_loss: 0.1736 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.17672 to 0.17360, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 894us/step - loss: 0.2667 - precision: 0.9182 - val_loss: 0.1702 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.17360 to 0.17015, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 930us/step - loss: 0.2565 - precision: 0.9182 - val_loss: 0.1781 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.17015\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 869us/step - loss: 0.2424 - precision: 0.9434 - val_loss: 0.1878 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.17015\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 956us/step - loss: 0.2432 - precision: 0.9560 - val_loss: 0.2020 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.17015\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 870us/step - loss: 0.2302 - precision: 0.9497 - val_loss: 0.2041 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.17015\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 855us/step - loss: 0.2153 - precision: 0.9560 - val_loss: 0.2051 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.17015\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 891us/step - loss: 0.2217 - precision: 0.9371 - val_loss: 0.2074 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.17015\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 930us/step - loss: 0.1936 - precision: 0.9245 - val_loss: 0.2187 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.17015\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 862us/step - loss: 0.2008 - precision: 0.9371 - val_loss: 0.2266 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.17015\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 912us/step - loss: 0.1730 - precision: 0.9623 - val_loss: 0.2485 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.17015\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 915us/step - loss: 0.1820 - precision: 0.9748 - val_loss: 0.2533 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.17015\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 909us/step - loss: 0.1548 - precision: 0.9560 - val_loss: 0.2622 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.17015\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 887us/step - loss: 0.1609 - precision: 0.9623 - val_loss: 0.2816 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.17015\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 879us/step - loss: 0.1473 - precision: 0.9748 - val_loss: 0.2850 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.17015\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 891us/step - loss: 0.1322 - precision: 0.9811 - val_loss: 0.3022 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.17015\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 927us/step - loss: 0.1301 - precision: 0.9937 - val_loss: 0.3075 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.17015\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 921us/step - loss: 0.1337 - precision: 0.9748 - val_loss: 0.3106 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.17015\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 947us/step - loss: 0.1339 - precision: 0.9686 - val_loss: 0.3157 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.17015\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 863us/step - loss: 0.1178 - precision: 0.9748 - val_loss: 0.3263 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.17015\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 870us/step - loss: 0.1195 - precision: 0.9748 - val_loss: 0.3437 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.17015\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 961us/step - loss: 0.1360 - precision: 0.9748 - val_loss: 0.3690 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.17015\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 889us/step - loss: 0.1226 - precision: 0.9623 - val_loss: 0.3753 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.17015\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 913us/step - loss: 0.1001 - precision: 0.9937 - val_loss: 0.3694 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.17015\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 870us/step - loss: 0.1136 - precision: 0.9623 - val_loss: 0.3751 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.17015\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 961us/step - loss: 0.1135 - precision: 0.9686 - val_loss: 0.4119 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.17015\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 902us/step - loss: 0.1153 - precision: 0.9811 - val_loss: 0.4353 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.17015\n",
            "119/119 [==============================] - 0s 3ms/step\n",
            "{'0': {'precision': 0.921875, 'recall': 1.0, 'f1-score': 0.959349593495935, 'support': 59}, '1': {'precision': 1.0, 'recall': 0.9166666666666666, 'f1-score': 0.9565217391304348, 'support': 60}, 'accuracy': 0.957983193277311, 'macro avg': {'precision': 0.9609375, 'recall': 0.9583333333333333, 'f1-score': 0.9579356663131848, 'support': 119}, 'weighted avg': {'precision': 0.961265756302521, 'recall': 0.957983193277311, 'f1-score': 0.9579237845721533, 'support': 119}}\n",
            "3\n",
            "-----------Split_Number--------- 4\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "159/159 [==============================] - 1s 8ms/step - loss: 0.6965 - precision: 0.5190 - val_loss: 0.6901 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69006, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 926us/step - loss: 0.6794 - precision: 0.5031 - val_loss: 0.6901 - val_precision: 0.4444\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69006\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 903us/step - loss: 0.6281 - precision: 0.6352 - val_loss: 0.5657 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69006 to 0.56567, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 833us/step - loss: 0.5721 - precision: 0.7044 - val_loss: 0.8312 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.56567\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 992us/step - loss: 0.5105 - precision: 0.7547 - val_loss: 0.9639 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.56567\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 894us/step - loss: 0.4878 - precision: 0.7987 - val_loss: 0.8266 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.56567\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 888us/step - loss: 0.4509 - precision: 0.7736 - val_loss: 0.8315 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.56567\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 851us/step - loss: 0.4271 - precision: 0.7736 - val_loss: 0.9317 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.56567\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 922us/step - loss: 0.3969 - precision: 0.8239 - val_loss: 0.7942 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.56567\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 821us/step - loss: 0.3630 - precision: 0.8553 - val_loss: 1.0950 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.56567\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 881us/step - loss: 0.3996 - precision: 0.8239 - val_loss: 0.8823 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.56567\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 958us/step - loss: 0.3311 - precision: 0.8491 - val_loss: 0.9806 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.56567\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 882us/step - loss: 0.3131 - precision: 0.8553 - val_loss: 0.8067 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.56567\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 850us/step - loss: 0.2764 - precision: 0.9057 - val_loss: 1.1640 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.56567\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 888us/step - loss: 0.2232 - precision: 0.8994 - val_loss: 1.5662 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.56567\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 911us/step - loss: 0.2493 - precision: 0.8994 - val_loss: 1.4559 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.56567\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 919us/step - loss: 0.2767 - precision: 0.8616 - val_loss: 0.8350 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.56567\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 912us/step - loss: 0.2367 - precision: 0.9119 - val_loss: 0.7089 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.56567\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 982us/step - loss: 0.2082 - precision: 0.8994 - val_loss: 0.7662 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.56567\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 903us/step - loss: 0.2006 - precision: 0.9182 - val_loss: 0.9179 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.56567\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 924us/step - loss: 0.1600 - precision: 0.9497 - val_loss: 0.9773 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.56567\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 880us/step - loss: 0.1871 - precision: 0.9245 - val_loss: 1.0209 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.56567\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 906us/step - loss: 0.1826 - precision: 0.8994 - val_loss: 1.0650 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.56567\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 915us/step - loss: 0.1375 - precision: 0.9623 - val_loss: 1.0830 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.56567\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 895us/step - loss: 0.2030 - precision: 0.9057 - val_loss: 1.1438 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.56567\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 920us/step - loss: 0.1673 - precision: 0.9371 - val_loss: 0.9947 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.56567\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 855us/step - loss: 0.1538 - precision: 0.9371 - val_loss: 0.8764 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.56567\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 896us/step - loss: 0.1313 - precision: 0.9560 - val_loss: 0.8022 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.56567\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 928us/step - loss: 0.0993 - precision: 0.9874 - val_loss: 1.0532 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.56567\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 910us/step - loss: 0.1169 - precision: 0.9308 - val_loss: 1.0839 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.56567\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 907us/step - loss: 0.1184 - precision: 0.9497 - val_loss: 1.1803 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.56567\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 907us/step - loss: 0.0922 - precision: 0.9623 - val_loss: 1.6569 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.56567\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 941us/step - loss: 0.0874 - precision: 0.9874 - val_loss: 1.9038 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.56567\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 913us/step - loss: 0.0621 - precision: 0.9874 - val_loss: 2.0286 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.56567\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 863us/step - loss: 0.1096 - precision: 0.9497 - val_loss: 1.3515 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.56567\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 968us/step - loss: 0.0982 - precision: 0.9560 - val_loss: 0.8899 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.56567\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 929us/step - loss: 0.0972 - precision: 0.9686 - val_loss: 1.0654 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.56567\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 906us/step - loss: 0.0905 - precision: 0.9686 - val_loss: 1.4069 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.56567\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 930us/step - loss: 0.0527 - precision: 0.9937 - val_loss: 1.5450 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.56567\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 892us/step - loss: 0.0849 - precision: 0.9748 - val_loss: 1.5648 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.56567\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 897us/step - loss: 0.0888 - precision: 0.9686 - val_loss: 1.5411 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.56567\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 894us/step - loss: 0.0722 - precision: 0.9748 - val_loss: 1.5834 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.56567\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 897us/step - loss: 0.0734 - precision: 0.9686 - val_loss: 1.6401 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.56567\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 914us/step - loss: 0.0571 - precision: 0.9937 - val_loss: 1.6802 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.56567\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 979us/step - loss: 0.0647 - precision: 0.9748 - val_loss: 1.8088 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.56567\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 936us/step - loss: 0.0912 - precision: 0.9560 - val_loss: 1.6617 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.56567\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 868us/step - loss: 0.0597 - precision: 0.9811 - val_loss: 1.6562 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.56567\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 934us/step - loss: 0.0480 - precision: 0.9874 - val_loss: 2.2335 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.56567\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 889us/step - loss: 0.0533 - precision: 0.9811 - val_loss: 2.7287 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.56567\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 909us/step - loss: 0.0568 - precision: 0.9748 - val_loss: 2.7898 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.56567\n",
            "119/119 [==============================] - 0s 3ms/step\n",
            "{'0': {'precision': 0.6666666666666666, 'recall': 0.8813559322033898, 'f1-score': 0.7591240875912408, 'support': 59}, '1': {'precision': 0.8292682926829268, 'recall': 0.5666666666666667, 'f1-score': 0.6732673267326732, 'support': 60}, 'accuracy': 0.7226890756302521, 'macro avg': {'precision': 0.7479674796747967, 'recall': 0.7240112994350283, 'f1-score': 0.716195707161957, 'support': 119}, 'weighted avg': {'precision': 0.7486506797841087, 'recall': 0.7226890756302521, 'f1-score': 0.715834964469274, 'support': 119}}\n",
            "4\n",
            "-----------Split_Number--------- 5\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "159/159 [==============================] - 2s 10ms/step - loss: 0.7367 - precision: 0.5251 - val_loss: 0.4664 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.46642, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 901us/step - loss: 0.6164 - precision: 0.6792 - val_loss: 0.4380 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.46642 to 0.43799, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 938us/step - loss: 0.6344 - precision: 0.6981 - val_loss: 0.4810 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.43799\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 906us/step - loss: 0.5780 - precision: 0.6667 - val_loss: 0.3254 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.43799 to 0.32540, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 875us/step - loss: 0.5335 - precision: 0.7233 - val_loss: 0.2821 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.32540 to 0.28210, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 877us/step - loss: 0.5134 - precision: 0.6981 - val_loss: 0.4385 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.28210\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 908us/step - loss: 0.5519 - precision: 0.6981 - val_loss: 0.2995 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.28210\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 881us/step - loss: 0.4696 - precision: 0.7296 - val_loss: 0.3054 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.28210\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 932us/step - loss: 0.4399 - precision: 0.7547 - val_loss: 0.2444 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.28210 to 0.24435, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 951us/step - loss: 0.4114 - precision: 0.7610 - val_loss: 0.2476 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.24435\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 855us/step - loss: 0.3696 - precision: 0.8050 - val_loss: 0.2455 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.24435\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 857us/step - loss: 0.3489 - precision: 0.7862 - val_loss: 0.2701 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.24435\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 915us/step - loss: 0.3385 - precision: 0.8679 - val_loss: 0.2325 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.24435 to 0.23245, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 876us/step - loss: 0.2811 - precision: 0.8994 - val_loss: 0.3413 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.23245\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 917us/step - loss: 0.2875 - precision: 0.8553 - val_loss: 0.3537 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.23245\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 910us/step - loss: 0.2737 - precision: 0.8742 - val_loss: 0.3691 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.23245\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 960us/step - loss: 0.2595 - precision: 0.8616 - val_loss: 0.4565 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.23245\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 904us/step - loss: 0.2449 - precision: 0.9057 - val_loss: 0.3334 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.23245\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 928us/step - loss: 0.2477 - precision: 0.8931 - val_loss: 0.3960 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.23245\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 938us/step - loss: 0.2526 - precision: 0.8868 - val_loss: 0.5233 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.23245\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 961us/step - loss: 0.2320 - precision: 0.9119 - val_loss: 0.4101 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.23245\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 907us/step - loss: 0.2113 - precision: 0.9308 - val_loss: 0.4519 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.23245\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 831us/step - loss: 0.2116 - precision: 0.9182 - val_loss: 0.4833 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.23245\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 958us/step - loss: 0.2162 - precision: 0.8931 - val_loss: 0.4577 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.23245\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 881us/step - loss: 0.1674 - precision: 0.9560 - val_loss: 0.5273 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.23245\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 927us/step - loss: 0.1305 - precision: 0.9623 - val_loss: 0.5546 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.23245\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 890us/step - loss: 0.1648 - precision: 0.9497 - val_loss: 0.3772 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.23245\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 901us/step - loss: 0.1562 - precision: 0.9560 - val_loss: 0.3662 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.23245\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 905us/step - loss: 0.2351 - precision: 0.8931 - val_loss: 0.5653 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.23245\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 912us/step - loss: 0.1652 - precision: 0.9623 - val_loss: 0.3569 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.23245\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 978us/step - loss: 0.1730 - precision: 0.9308 - val_loss: 0.3575 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.23245\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 942us/step - loss: 0.1558 - precision: 0.9560 - val_loss: 0.7008 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.23245\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 895us/step - loss: 0.1591 - precision: 0.9497 - val_loss: 0.4595 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.23245\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 921us/step - loss: 0.1168 - precision: 0.9748 - val_loss: 0.3301 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.23245\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 939us/step - loss: 0.1135 - precision: 0.9811 - val_loss: 0.3642 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.23245\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 919us/step - loss: 0.1258 - precision: 0.9623 - val_loss: 0.4365 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.23245\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 930us/step - loss: 0.0836 - precision: 0.9748 - val_loss: 0.4732 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.23245\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 950us/step - loss: 0.1201 - precision: 0.9686 - val_loss: 0.4191 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.23245\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 946us/step - loss: 0.1170 - precision: 0.9748 - val_loss: 0.3721 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.23245\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 948us/step - loss: 0.0762 - precision: 0.9937 - val_loss: 0.5868 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.23245\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 919us/step - loss: 0.0882 - precision: 0.9748 - val_loss: 0.7948 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.23245\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 876us/step - loss: 0.0704 - precision: 0.9811 - val_loss: 0.4938 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.23245\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 922us/step - loss: 0.0643 - precision: 0.9811 - val_loss: 0.6199 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.23245\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 920us/step - loss: 0.0974 - precision: 0.9686 - val_loss: 0.7565 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.23245\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 920us/step - loss: 0.0818 - precision: 0.9811 - val_loss: 0.2319 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.23245 to 0.23194, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 836us/step - loss: 0.0952 - precision: 0.9811 - val_loss: 0.0846 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.23194 to 0.08461, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 896us/step - loss: 0.0600 - precision: 0.9811 - val_loss: 0.1186 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.08461\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 949us/step - loss: 0.0830 - precision: 0.9623 - val_loss: 0.1410 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.08461\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 918us/step - loss: 0.0817 - precision: 0.9748 - val_loss: 0.0707 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.08461 to 0.07074, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 934us/step - loss: 0.0732 - precision: 0.9811 - val_loss: 0.1216 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.07074\n",
            "119/119 [==============================] - 0s 4ms/step\n",
            "{'0': {'precision': 0.8928571428571429, 'recall': 0.847457627118644, 'f1-score': 0.8695652173913044, 'support': 59}, '1': {'precision': 0.8571428571428571, 'recall': 0.9, 'f1-score': 0.8780487804878048, 'support': 60}, 'accuracy': 0.8739495798319328, 'macro avg': {'precision': 0.875, 'recall': 0.873728813559322, 'f1-score': 0.8738069989395546, 'support': 119}, 'weighted avg': {'precision': 0.8748499399759905, 'recall': 0.8739495798319328, 'f1-score': 0.873842644162649, 'support': 119}}\n",
            "5\n",
            "-----------Split_Number--------- 6\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "159/159 [==============================] - 2s 12ms/step - loss: 0.7116 - precision: 0.5220 - val_loss: 0.5930 - val_precision: 0.4444\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59302, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 909us/step - loss: 0.6356 - precision: 0.6101 - val_loss: 0.5809 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59302 to 0.58092, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 860us/step - loss: 0.5786 - precision: 0.6981 - val_loss: 0.4446 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.58092 to 0.44458, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 897us/step - loss: 0.5619 - precision: 0.7044 - val_loss: 0.4254 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.44458 to 0.42539, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 894us/step - loss: 0.5337 - precision: 0.7358 - val_loss: 0.4145 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.42539 to 0.41449, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 889us/step - loss: 0.5301 - precision: 0.7421 - val_loss: 0.4346 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.41449\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 883us/step - loss: 0.4682 - precision: 0.8176 - val_loss: 0.3983 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.41449 to 0.39827, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 944us/step - loss: 0.4533 - precision: 0.7987 - val_loss: 0.3630 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.39827 to 0.36303, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 904us/step - loss: 0.4119 - precision: 0.8491 - val_loss: 0.3560 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.36303 to 0.35597, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 974us/step - loss: 0.3920 - precision: 0.8616 - val_loss: 0.4065 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.35597\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 924us/step - loss: 0.3688 - precision: 0.8553 - val_loss: 0.4897 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.35597\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 910us/step - loss: 0.3381 - precision: 0.8553 - val_loss: 0.4340 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.35597\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 947us/step - loss: 0.3316 - precision: 0.8491 - val_loss: 0.4379 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.35597\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 919us/step - loss: 0.3006 - precision: 0.8931 - val_loss: 0.3623 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.35597\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 900us/step - loss: 0.2870 - precision: 0.8491 - val_loss: 0.5426 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.35597\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 892us/step - loss: 0.2902 - precision: 0.8742 - val_loss: 0.6701 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.35597\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 944us/step - loss: 0.2696 - precision: 0.8742 - val_loss: 0.6175 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.35597\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 968us/step - loss: 0.2178 - precision: 0.9119 - val_loss: 0.7788 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.35597\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 921us/step - loss: 0.2200 - precision: 0.8994 - val_loss: 0.7158 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.35597\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 943us/step - loss: 0.2608 - precision: 0.8805 - val_loss: 0.4217 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.35597\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 915us/step - loss: 0.2137 - precision: 0.9308 - val_loss: 0.6767 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.35597\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 927us/step - loss: 0.2108 - precision: 0.9057 - val_loss: 1.1496 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.35597\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 911us/step - loss: 0.2030 - precision: 0.9057 - val_loss: 0.6656 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.35597\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 948us/step - loss: 0.2109 - precision: 0.9245 - val_loss: 1.0678 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.35597\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 938us/step - loss: 0.2028 - precision: 0.9119 - val_loss: 0.9983 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.35597\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 892us/step - loss: 0.1844 - precision: 0.9434 - val_loss: 0.5794 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.35597\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 960us/step - loss: 0.1714 - precision: 0.9119 - val_loss: 0.2818 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.35597 to 0.28176, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 893us/step - loss: 0.1712 - precision: 0.9308 - val_loss: 0.2572 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.28176 to 0.25721, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 971us/step - loss: 0.1491 - precision: 0.9560 - val_loss: 0.5666 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.25721\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 978us/step - loss: 0.1635 - precision: 0.9245 - val_loss: 0.3763 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.25721\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 976us/step - loss: 0.1366 - precision: 0.9371 - val_loss: 0.9599 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.25721\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 926us/step - loss: 0.1225 - precision: 0.9434 - val_loss: 1.4442 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.25721\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 933us/step - loss: 0.1340 - precision: 0.9560 - val_loss: 1.4253 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.25721\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 940us/step - loss: 0.1289 - precision: 0.9434 - val_loss: 1.1695 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.25721\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 915us/step - loss: 0.1073 - precision: 0.9686 - val_loss: 1.6808 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.25721\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.1733 - precision: 0.9497 - val_loss: 1.2426 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.25721\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 946us/step - loss: 0.1024 - precision: 0.9748 - val_loss: 0.9032 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.25721\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 915us/step - loss: 0.1144 - precision: 0.9560 - val_loss: 0.9933 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.25721\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 957us/step - loss: 0.1674 - precision: 0.9623 - val_loss: 0.9261 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.25721\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 909us/step - loss: 0.1153 - precision: 0.9623 - val_loss: 0.3033 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.25721\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.1076 - precision: 0.9623 - val_loss: 0.4058 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.25721\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 929us/step - loss: 0.1179 - precision: 0.9497 - val_loss: 0.6359 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.25721\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 994us/step - loss: 0.0871 - precision: 0.9937 - val_loss: 0.9891 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.25721\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 942us/step - loss: 0.0793 - precision: 1.0000 - val_loss: 1.3768 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.25721\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 962us/step - loss: 0.0847 - precision: 0.9748 - val_loss: 1.1408 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.25721\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 967us/step - loss: 0.0840 - precision: 0.9560 - val_loss: 1.3294 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.25721\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 956us/step - loss: 0.1100 - precision: 0.9686 - val_loss: 1.1697 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.25721\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 999us/step - loss: 0.0723 - precision: 0.9874 - val_loss: 1.0514 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.25721\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 967us/step - loss: 0.0565 - precision: 1.0000 - val_loss: 0.9566 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.25721\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 920us/step - loss: 0.0630 - precision: 0.9874 - val_loss: 0.9304 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.25721\n",
            "119/119 [==============================] - 1s 5ms/step\n",
            "{'0': {'precision': 0.8775510204081632, 'recall': 0.7288135593220338, 'f1-score': 0.7962962962962963, 'support': 59}, '1': {'precision': 0.7714285714285715, 'recall': 0.9, 'f1-score': 0.8307692307692307, 'support': 60}, 'accuracy': 0.8151260504201681, 'macro avg': {'precision': 0.8244897959183674, 'recall': 0.8144067796610169, 'f1-score': 0.8135327635327635, 'support': 119}, 'weighted avg': {'precision': 0.824043903275596, 'recall': 0.8151260504201681, 'f1-score': 0.8136776077952548, 'support': 119}}\n",
            "6\n",
            "-----------Split_Number--------- 7\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(159, 768) (159, 31) 159\n",
            "Train on 159 samples, validate on 18 samples\n",
            "Epoch 1/50\n",
            "159/159 [==============================] - 2s 14ms/step - loss: 0.6867 - precision: 0.4858 - val_loss: 0.6868 - val_precision: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68675, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "159/159 [==============================] - 0s 945us/step - loss: 0.6582 - precision: 0.5094 - val_loss: 0.7023 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.68675\n",
            "Epoch 3/50\n",
            "159/159 [==============================] - 0s 997us/step - loss: 0.6276 - precision: 0.6226 - val_loss: 0.6695 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68675 to 0.66954, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "159/159 [==============================] - 0s 927us/step - loss: 0.6010 - precision: 0.6226 - val_loss: 0.8459 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.66954\n",
            "Epoch 5/50\n",
            "159/159 [==============================] - 0s 999us/step - loss: 0.5872 - precision: 0.6541 - val_loss: 0.7999 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.66954\n",
            "Epoch 6/50\n",
            "159/159 [==============================] - 0s 950us/step - loss: 0.5466 - precision: 0.7044 - val_loss: 0.6300 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.66954 to 0.63004, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 7/50\n",
            "159/159 [==============================] - 0s 967us/step - loss: 0.5198 - precision: 0.7610 - val_loss: 0.6011 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.63004 to 0.60113, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 8/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.4637 - precision: 0.7484 - val_loss: 0.8335 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.60113\n",
            "Epoch 9/50\n",
            "159/159 [==============================] - 0s 983us/step - loss: 0.4462 - precision: 0.7987 - val_loss: 1.0871 - val_precision: 0.5556\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.60113\n",
            "Epoch 10/50\n",
            "159/159 [==============================] - 0s 936us/step - loss: 0.4431 - precision: 0.7799 - val_loss: 0.9745 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.60113\n",
            "Epoch 11/50\n",
            "159/159 [==============================] - 0s 967us/step - loss: 0.3524 - precision: 0.8239 - val_loss: 0.9106 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.60113\n",
            "Epoch 12/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.3562 - precision: 0.8553 - val_loss: 0.9643 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.60113\n",
            "Epoch 13/50\n",
            "159/159 [==============================] - 0s 986us/step - loss: 0.3116 - precision: 0.8553 - val_loss: 1.0660 - val_precision: 0.6111\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.60113\n",
            "Epoch 14/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.3379 - precision: 0.8428 - val_loss: 1.2078 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.60113\n",
            "Epoch 15/50\n",
            "159/159 [==============================] - 0s 971us/step - loss: 0.2968 - precision: 0.8805 - val_loss: 0.8222 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.60113\n",
            "Epoch 16/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.2526 - precision: 0.9057 - val_loss: 0.7012 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.60113\n",
            "Epoch 17/50\n",
            "159/159 [==============================] - 0s 963us/step - loss: 0.2600 - precision: 0.8868 - val_loss: 0.8743 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.60113\n",
            "Epoch 18/50\n",
            "159/159 [==============================] - 0s 964us/step - loss: 0.2365 - precision: 0.8931 - val_loss: 1.0474 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.60113\n",
            "Epoch 19/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.2406 - precision: 0.9119 - val_loss: 0.7996 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.60113\n",
            "Epoch 20/50\n",
            "159/159 [==============================] - 0s 997us/step - loss: 0.2290 - precision: 0.9182 - val_loss: 0.5082 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.60113 to 0.50821, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 21/50\n",
            "159/159 [==============================] - 0s 886us/step - loss: 0.1771 - precision: 0.9119 - val_loss: 0.5774 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.50821\n",
            "Epoch 22/50\n",
            "159/159 [==============================] - 0s 1000us/step - loss: 0.2182 - precision: 0.8931 - val_loss: 0.9365 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.50821\n",
            "Epoch 23/50\n",
            "159/159 [==============================] - 0s 924us/step - loss: 0.2070 - precision: 0.9245 - val_loss: 1.1472 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.50821\n",
            "Epoch 24/50\n",
            "159/159 [==============================] - 0s 987us/step - loss: 0.1687 - precision: 0.9308 - val_loss: 0.4944 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.50821 to 0.49442, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 25/50\n",
            "159/159 [==============================] - 0s 933us/step - loss: 0.1945 - precision: 0.9245 - val_loss: 0.5962 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.49442\n",
            "Epoch 26/50\n",
            "159/159 [==============================] - 0s 946us/step - loss: 0.1527 - precision: 0.9623 - val_loss: 0.5410 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.49442\n",
            "Epoch 27/50\n",
            "159/159 [==============================] - 0s 968us/step - loss: 0.1319 - precision: 0.9686 - val_loss: 0.5018 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.49442\n",
            "Epoch 28/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.1723 - precision: 0.9434 - val_loss: 0.9771 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.49442\n",
            "Epoch 29/50\n",
            "159/159 [==============================] - 0s 962us/step - loss: 0.1597 - precision: 0.9497 - val_loss: 0.8033 - val_precision: 0.6667\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.49442\n",
            "Epoch 30/50\n",
            "159/159 [==============================] - 0s 983us/step - loss: 0.1347 - precision: 0.9623 - val_loss: 0.5847 - val_precision: 0.7222\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.49442\n",
            "Epoch 31/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.1334 - precision: 0.9497 - val_loss: 0.9822 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.49442\n",
            "Epoch 32/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.0927 - precision: 0.9811 - val_loss: 1.0371 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.49442\n",
            "Epoch 33/50\n",
            "159/159 [==============================] - 0s 974us/step - loss: 0.1108 - precision: 0.9748 - val_loss: 0.5675 - val_precision: 0.7778\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.49442\n",
            "Epoch 34/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.1351 - precision: 0.9748 - val_loss: 0.3257 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.49442 to 0.32567, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 35/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.1136 - precision: 0.9748 - val_loss: 0.8165 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.32567\n",
            "Epoch 36/50\n",
            "159/159 [==============================] - 0s 926us/step - loss: 0.1082 - precision: 0.9811 - val_loss: 0.7022 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.32567\n",
            "Epoch 37/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.0666 - precision: 1.0000 - val_loss: 0.8356 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.32567\n",
            "Epoch 38/50\n",
            "159/159 [==============================] - 0s 965us/step - loss: 0.0808 - precision: 0.9874 - val_loss: 1.1553 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.32567\n",
            "Epoch 39/50\n",
            "159/159 [==============================] - 0s 948us/step - loss: 0.1059 - precision: 0.9748 - val_loss: 0.9748 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.32567\n",
            "Epoch 40/50\n",
            "159/159 [==============================] - 0s 980us/step - loss: 0.0594 - precision: 0.9874 - val_loss: 0.8136 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.32567\n",
            "Epoch 41/50\n",
            "159/159 [==============================] - 0s 942us/step - loss: 0.1111 - precision: 0.9623 - val_loss: 0.6599 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.32567\n",
            "Epoch 42/50\n",
            "159/159 [==============================] - 0s 958us/step - loss: 0.0364 - precision: 0.9937 - val_loss: 0.5438 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.32567\n",
            "Epoch 43/50\n",
            "159/159 [==============================] - 0s 970us/step - loss: 0.0581 - precision: 0.9811 - val_loss: 0.5283 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.32567\n",
            "Epoch 44/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.0748 - precision: 0.9748 - val_loss: 0.6663 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.32567\n",
            "Epoch 45/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.0457 - precision: 0.9937 - val_loss: 0.7719 - val_precision: 0.8333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.32567\n",
            "Epoch 46/50\n",
            "159/159 [==============================] - 0s 998us/step - loss: 0.0838 - precision: 0.9811 - val_loss: 0.7630 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.32567\n",
            "Epoch 47/50\n",
            "159/159 [==============================] - 0s 997us/step - loss: 0.0687 - precision: 0.9874 - val_loss: 0.1361 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.32567 to 0.13605, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 48/50\n",
            "159/159 [==============================] - 0s 921us/step - loss: 0.0529 - precision: 0.9874 - val_loss: 0.3239 - val_precision: 0.8889\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.13605\n",
            "Epoch 49/50\n",
            "159/159 [==============================] - 0s 1ms/step - loss: 0.0672 - precision: 0.9811 - val_loss: 0.1258 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.13605 to 0.12577, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 50/50\n",
            "159/159 [==============================] - 0s 958us/step - loss: 0.0587 - precision: 0.9937 - val_loss: 0.1130 - val_precision: 0.9444\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.12577 to 0.11303, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "119/119 [==============================] - 1s 6ms/step\n",
            "{'0': {'precision': 0.8679245283018868, 'recall': 0.7796610169491526, 'f1-score': 0.8214285714285715, 'support': 59}, '1': {'precision': 0.803030303030303, 'recall': 0.8833333333333333, 'f1-score': 0.8412698412698413, 'support': 60}, 'accuracy': 0.8319327731092437, 'macro avg': {'precision': 0.8354774156660949, 'recall': 0.831497175141243, 'f1-score': 0.8313492063492064, 'support': 119}, 'weighted avg': {'precision': 0.8352047508540293, 'recall': 0.8319327731092437, 'f1-score': 0.8314325730292118, 'support': 119}}\n",
            "7\n",
            "-----------Split_Number--------- 8\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(160, 768) (160, 31) 160\n",
            "Train on 160 samples, validate on 17 samples\n",
            "Epoch 1/50\n",
            "160/160 [==============================] - 2s 15ms/step - loss: 0.7341 - precision: 0.5663 - val_loss: 0.3778 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.37781, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "160/160 [==============================] - 0s 918us/step - loss: 0.5776 - precision: 0.7125 - val_loss: 0.3803 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.37781\n",
            "Epoch 3/50\n",
            "160/160 [==============================] - 0s 928us/step - loss: 0.5393 - precision: 0.7625 - val_loss: 0.3060 - val_precision: 0.9412\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.37781 to 0.30597, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "160/160 [==============================] - 0s 953us/step - loss: 0.5024 - precision: 0.7812 - val_loss: 0.5309 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.30597\n",
            "Epoch 5/50\n",
            "160/160 [==============================] - 0s 938us/step - loss: 0.5367 - precision: 0.7750 - val_loss: 0.3389 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.30597\n",
            "Epoch 6/50\n",
            "160/160 [==============================] - 0s 991us/step - loss: 0.4633 - precision: 0.7875 - val_loss: 0.2736 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.30597 to 0.27359, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 7/50\n",
            "160/160 [==============================] - 0s 903us/step - loss: 0.4066 - precision: 0.8125 - val_loss: 0.3638 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.27359\n",
            "Epoch 8/50\n",
            "160/160 [==============================] - 0s 954us/step - loss: 0.3665 - precision: 0.8625 - val_loss: 0.3522 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.27359\n",
            "Epoch 9/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3745 - precision: 0.8438 - val_loss: 0.4731 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.27359\n",
            "Epoch 10/50\n",
            "160/160 [==============================] - 0s 961us/step - loss: 0.3694 - precision: 0.8312 - val_loss: 0.5014 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.27359\n",
            "Epoch 11/50\n",
            "160/160 [==============================] - 0s 950us/step - loss: 0.2814 - precision: 0.8937 - val_loss: 0.9857 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27359\n",
            "Epoch 12/50\n",
            "160/160 [==============================] - 0s 939us/step - loss: 0.3282 - precision: 0.8500 - val_loss: 0.6149 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.27359\n",
            "Epoch 13/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2829 - precision: 0.8875 - val_loss: 0.4922 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.27359\n",
            "Epoch 14/50\n",
            "160/160 [==============================] - 0s 890us/step - loss: 0.2435 - precision: 0.9062 - val_loss: 0.7671 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.27359\n",
            "Epoch 15/50\n",
            "160/160 [==============================] - 0s 935us/step - loss: 0.2680 - precision: 0.8812 - val_loss: 0.7395 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.27359\n",
            "Epoch 16/50\n",
            "160/160 [==============================] - 0s 936us/step - loss: 0.2494 - precision: 0.9000 - val_loss: 0.6803 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.27359\n",
            "Epoch 17/50\n",
            "160/160 [==============================] - 0s 925us/step - loss: 0.2165 - precision: 0.9250 - val_loss: 0.8256 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27359\n",
            "Epoch 18/50\n",
            "160/160 [==============================] - 0s 899us/step - loss: 0.1977 - precision: 0.9500 - val_loss: 1.0736 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.27359\n",
            "Epoch 19/50\n",
            "160/160 [==============================] - 0s 998us/step - loss: 0.1678 - precision: 0.9562 - val_loss: 1.0793 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.27359\n",
            "Epoch 20/50\n",
            "160/160 [==============================] - 0s 967us/step - loss: 0.1618 - precision: 0.9437 - val_loss: 1.0889 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27359\n",
            "Epoch 21/50\n",
            "160/160 [==============================] - 0s 963us/step - loss: 0.1650 - precision: 0.9250 - val_loss: 1.1466 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.27359\n",
            "Epoch 22/50\n",
            "160/160 [==============================] - 0s 941us/step - loss: 0.1595 - precision: 0.9313 - val_loss: 1.2098 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.27359\n",
            "Epoch 23/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1473 - precision: 0.9437 - val_loss: 1.2645 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.27359\n",
            "Epoch 24/50\n",
            "160/160 [==============================] - 0s 923us/step - loss: 0.1181 - precision: 0.9562 - val_loss: 1.3322 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.27359\n",
            "Epoch 25/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1373 - precision: 0.9500 - val_loss: 1.1046 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.27359\n",
            "Epoch 26/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1137 - precision: 0.9625 - val_loss: 1.0615 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.27359\n",
            "Epoch 27/50\n",
            "160/160 [==============================] - 0s 990us/step - loss: 0.1480 - precision: 0.9375 - val_loss: 0.8916 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.27359\n",
            "Epoch 28/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1065 - precision: 0.9625 - val_loss: 1.1401 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.27359\n",
            "Epoch 29/50\n",
            "160/160 [==============================] - 0s 903us/step - loss: 0.1256 - precision: 0.9500 - val_loss: 1.2517 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.27359\n",
            "Epoch 30/50\n",
            "160/160 [==============================] - 0s 933us/step - loss: 0.0944 - precision: 0.9562 - val_loss: 1.3014 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.27359\n",
            "Epoch 31/50\n",
            "160/160 [==============================] - 0s 950us/step - loss: 0.1205 - precision: 0.9313 - val_loss: 1.4001 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.27359\n",
            "Epoch 32/50\n",
            "160/160 [==============================] - 0s 940us/step - loss: 0.0793 - precision: 0.9750 - val_loss: 1.3405 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.27359\n",
            "Epoch 33/50\n",
            "160/160 [==============================] - 0s 987us/step - loss: 0.1319 - precision: 0.9500 - val_loss: 1.3441 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.27359\n",
            "Epoch 34/50\n",
            "160/160 [==============================] - 0s 968us/step - loss: 0.1151 - precision: 0.9437 - val_loss: 1.4126 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.27359\n",
            "Epoch 35/50\n",
            "160/160 [==============================] - 0s 969us/step - loss: 0.0889 - precision: 0.9813 - val_loss: 1.2846 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.27359\n",
            "Epoch 36/50\n",
            "160/160 [==============================] - 0s 978us/step - loss: 0.1065 - precision: 0.9625 - val_loss: 1.2947 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.27359\n",
            "Epoch 37/50\n",
            "160/160 [==============================] - 0s 967us/step - loss: 0.0707 - precision: 0.9813 - val_loss: 1.2480 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.27359\n",
            "Epoch 38/50\n",
            "160/160 [==============================] - 0s 969us/step - loss: 0.1179 - precision: 0.9375 - val_loss: 1.1141 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.27359\n",
            "Epoch 39/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0829 - precision: 0.9750 - val_loss: 1.1552 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.27359\n",
            "Epoch 40/50\n",
            "160/160 [==============================] - 0s 990us/step - loss: 0.0930 - precision: 0.9625 - val_loss: 1.2212 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.27359\n",
            "Epoch 41/50\n",
            "160/160 [==============================] - 0s 962us/step - loss: 0.0708 - precision: 0.9688 - val_loss: 1.1048 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.27359\n",
            "Epoch 42/50\n",
            "160/160 [==============================] - 0s 978us/step - loss: 0.0954 - precision: 0.9688 - val_loss: 1.0429 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.27359\n",
            "Epoch 43/50\n",
            "160/160 [==============================] - 0s 926us/step - loss: 0.1197 - precision: 0.9437 - val_loss: 1.0326 - val_precision: 0.9412\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.27359\n",
            "Epoch 44/50\n",
            "160/160 [==============================] - 0s 969us/step - loss: 0.0946 - precision: 0.9750 - val_loss: 1.1469 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.27359\n",
            "Epoch 45/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0940 - precision: 0.9750 - val_loss: 1.0486 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.27359\n",
            "Epoch 46/50\n",
            "160/160 [==============================] - 0s 991us/step - loss: 0.0438 - precision: 0.9875 - val_loss: 1.0304 - val_precision: 0.9412\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27359\n",
            "Epoch 47/50\n",
            "160/160 [==============================] - 0s 971us/step - loss: 0.0603 - precision: 0.9750 - val_loss: 1.0497 - val_precision: 0.9412\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.27359\n",
            "Epoch 48/50\n",
            "160/160 [==============================] - 0s 970us/step - loss: 0.0781 - precision: 0.9688 - val_loss: 1.0358 - val_precision: 0.9412\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27359\n",
            "Epoch 49/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0722 - precision: 0.9688 - val_loss: 1.0754 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.27359\n",
            "Epoch 50/50\n",
            "160/160 [==============================] - 0s 992us/step - loss: 0.0533 - precision: 0.9813 - val_loss: 1.0376 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.27359\n",
            "119/119 [==============================] - 1s 6ms/step\n",
            "{'0': {'precision': 0.7872340425531915, 'recall': 0.6271186440677966, 'f1-score': 0.6981132075471698, 'support': 59}, '1': {'precision': 0.6944444444444444, 'recall': 0.8333333333333334, 'f1-score': 0.7575757575757577, 'support': 60}, 'accuracy': 0.7310924369747899, 'macro avg': {'precision': 0.740839243498818, 'recall': 0.730225988700565, 'f1-score': 0.7278444825614637, 'support': 119}, 'weighted avg': {'precision': 0.7404493712378568, 'recall': 0.7310924369747899, 'f1-score': 0.7280943252086427, 'support': 119}}\n",
            "8\n",
            "-----------Split_Number--------- 9\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(160, 768) (160, 31) 160\n",
            "Train on 160 samples, validate on 17 samples\n",
            "Epoch 1/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.7495 - precision: 0.4982 - val_loss: 0.6913 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69134, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "160/160 [==============================] - 0s 927us/step - loss: 0.7285 - precision: 0.5375 - val_loss: 0.6572 - val_precision: 0.5294\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69134 to 0.65716, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 3/50\n",
            "160/160 [==============================] - 0s 942us/step - loss: 0.6368 - precision: 0.6750 - val_loss: 0.5770 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.65716 to 0.57701, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 4/50\n",
            "160/160 [==============================] - 0s 978us/step - loss: 0.6205 - precision: 0.6875 - val_loss: 0.6177 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.57701\n",
            "Epoch 5/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5675 - precision: 0.7375 - val_loss: 0.8455 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.57701\n",
            "Epoch 6/50\n",
            "160/160 [==============================] - 0s 991us/step - loss: 0.5617 - precision: 0.7500 - val_loss: 0.5417 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.57701 to 0.54167, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 7/50\n",
            "160/160 [==============================] - 0s 997us/step - loss: 0.5662 - precision: 0.7500 - val_loss: 0.5000 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.54167 to 0.49999, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 8/50\n",
            "160/160 [==============================] - 0s 922us/step - loss: 0.5202 - precision: 0.7625 - val_loss: 0.7229 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.49999\n",
            "Epoch 9/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5324 - precision: 0.7500 - val_loss: 0.5661 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.49999\n",
            "Epoch 10/50\n",
            "160/160 [==============================] - 0s 999us/step - loss: 0.4537 - precision: 0.8125 - val_loss: 0.7583 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.49999\n",
            "Epoch 11/50\n",
            "160/160 [==============================] - 0s 981us/step - loss: 0.4212 - precision: 0.8500 - val_loss: 0.9543 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.49999\n",
            "Epoch 12/50\n",
            "160/160 [==============================] - 0s 978us/step - loss: 0.4299 - precision: 0.8125 - val_loss: 0.6043 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.49999\n",
            "Epoch 13/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3886 - precision: 0.8375 - val_loss: 0.6758 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.49999\n",
            "Epoch 14/50\n",
            "160/160 [==============================] - 0s 995us/step - loss: 0.3857 - precision: 0.8438 - val_loss: 0.7582 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.49999\n",
            "Epoch 15/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3578 - precision: 0.8438 - val_loss: 0.6291 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.49999\n",
            "Epoch 16/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3364 - precision: 0.8438 - val_loss: 0.6625 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.49999\n",
            "Epoch 17/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2784 - precision: 0.9188 - val_loss: 0.5822 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.49999\n",
            "Epoch 18/50\n",
            "160/160 [==============================] - 0s 995us/step - loss: 0.2968 - precision: 0.8812 - val_loss: 0.7813 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.49999\n",
            "Epoch 19/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2712 - precision: 0.8812 - val_loss: 0.7962 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.49999\n",
            "Epoch 20/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2489 - precision: 0.9250 - val_loss: 0.7242 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.49999\n",
            "Epoch 21/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2366 - precision: 0.9000 - val_loss: 0.4413 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.49999 to 0.44133, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 22/50\n",
            "160/160 [==============================] - 0s 949us/step - loss: 0.2198 - precision: 0.9062 - val_loss: 0.5420 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.44133\n",
            "Epoch 23/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2288 - precision: 0.8937 - val_loss: 0.7982 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.44133\n",
            "Epoch 24/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2290 - precision: 0.9125 - val_loss: 0.5170 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.44133\n",
            "Epoch 25/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1762 - precision: 0.9375 - val_loss: 0.5548 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.44133\n",
            "Epoch 26/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2049 - precision: 0.9250 - val_loss: 0.7295 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.44133\n",
            "Epoch 27/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1546 - precision: 0.9437 - val_loss: 1.1917 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.44133\n",
            "Epoch 28/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1762 - precision: 0.9250 - val_loss: 0.8720 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.44133\n",
            "Epoch 29/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2017 - precision: 0.9125 - val_loss: 0.6277 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.44133\n",
            "Epoch 30/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2405 - precision: 0.8937 - val_loss: 1.0429 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.44133\n",
            "Epoch 31/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2212 - precision: 0.9313 - val_loss: 1.4507 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.44133\n",
            "Epoch 32/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1625 - precision: 0.9375 - val_loss: 0.7640 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.44133\n",
            "Epoch 33/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1356 - precision: 0.9375 - val_loss: 0.7625 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.44133\n",
            "Epoch 34/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1096 - precision: 0.9625 - val_loss: 0.8607 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.44133\n",
            "Epoch 35/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1441 - precision: 0.9313 - val_loss: 0.9610 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.44133\n",
            "Epoch 36/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1102 - precision: 0.9437 - val_loss: 1.1825 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.44133\n",
            "Epoch 37/50\n",
            "160/160 [==============================] - 0s 1000us/step - loss: 0.1287 - precision: 0.9500 - val_loss: 1.3456 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.44133\n",
            "Epoch 38/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1150 - precision: 0.9375 - val_loss: 1.2607 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.44133\n",
            "Epoch 39/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1246 - precision: 0.9625 - val_loss: 1.1514 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.44133\n",
            "Epoch 40/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0997 - precision: 0.9625 - val_loss: 1.3748 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.44133\n",
            "Epoch 41/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0819 - precision: 0.9688 - val_loss: 1.6234 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.44133\n",
            "Epoch 42/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1238 - precision: 0.9313 - val_loss: 1.5374 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.44133\n",
            "Epoch 43/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1026 - precision: 0.9375 - val_loss: 1.8058 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.44133\n",
            "Epoch 44/50\n",
            "160/160 [==============================] - 0s 987us/step - loss: 0.1448 - precision: 0.9313 - val_loss: 2.4663 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.44133\n",
            "Epoch 45/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1681 - precision: 0.9437 - val_loss: 0.9365 - val_precision: 0.8824\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.44133\n",
            "Epoch 46/50\n",
            "160/160 [==============================] - 0s 996us/step - loss: 0.0912 - precision: 0.9625 - val_loss: 1.1024 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.44133\n",
            "Epoch 47/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1129 - precision: 0.9562 - val_loss: 1.1787 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.44133\n",
            "Epoch 48/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0931 - precision: 0.9688 - val_loss: 1.3044 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.44133\n",
            "Epoch 49/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0996 - precision: 0.9688 - val_loss: 1.3568 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.44133\n",
            "Epoch 50/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0805 - precision: 0.9750 - val_loss: 1.4089 - val_precision: 0.8235\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.44133\n",
            "119/119 [==============================] - 1s 7ms/step\n",
            "{'0': {'precision': 0.7758620689655172, 'recall': 0.7627118644067796, 'f1-score': 0.7692307692307694, 'support': 59}, '1': {'precision': 0.7704918032786885, 'recall': 0.7833333333333333, 'f1-score': 0.7768595041322314, 'support': 60}, 'accuracy': 0.773109243697479, 'macro avg': {'precision': 0.7731769361221028, 'recall': 0.7730225988700565, 'f1-score': 0.7730451366815003, 'support': 119}, 'weighted avg': {'precision': 0.7731543719805615, 'recall': 0.773109243697479, 'f1-score': 0.7730771901894896, 'support': 119}}\n",
            "9\n",
            "-----------Split_Number--------- 10\n",
            "drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/\n",
            "True\n",
            "True\n",
            "Loading the pickle file from drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Model_Data/1_to_1/splits_final.pkl ...........\n",
            "Loaded the pickle File\n",
            "(119, 768) (119, 31) 119\n",
            "(160, 768) (160, 31) 160\n",
            "Train on 160 samples, validate on 17 samples\n",
            "Epoch 1/50\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.6950 - precision: 0.4375 - val_loss: 0.6438 - val_precision: 0.5294\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.64383, saving model to drive/MyDrive/ICWSM_2022/Weakening the Inner Strength/data/NURSE_data//Models/Prem_model11.pkl\n",
            "Epoch 2/50\n",
            "160/160 [==============================] - 0s 996us/step - loss: 0.6771 - precision: 0.4875 - val_loss: 0.6505 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.64383\n",
            "Epoch 3/50\n",
            "160/160 [==============================] - 0s 1000us/step - loss: 0.6025 - precision: 0.6250 - val_loss: 0.8909 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.64383\n",
            "Epoch 4/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5442 - precision: 0.7313 - val_loss: 0.7658 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.64383\n",
            "Epoch 5/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5318 - precision: 0.7875 - val_loss: 1.2559 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.64383\n",
            "Epoch 6/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5099 - precision: 0.7750 - val_loss: 1.1599 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.64383\n",
            "Epoch 7/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.5085 - precision: 0.7313 - val_loss: 1.1071 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.64383\n",
            "Epoch 8/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4974 - precision: 0.7375 - val_loss: 0.6773 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.64383\n",
            "Epoch 9/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4322 - precision: 0.8250 - val_loss: 0.8697 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.64383\n",
            "Epoch 10/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.4001 - precision: 0.8438 - val_loss: 1.0362 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.64383\n",
            "Epoch 11/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3535 - precision: 0.8125 - val_loss: 1.1914 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.64383\n",
            "Epoch 12/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3867 - precision: 0.8125 - val_loss: 1.4318 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.64383\n",
            "Epoch 13/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3460 - precision: 0.8875 - val_loss: 1.6818 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.64383\n",
            "Epoch 14/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3030 - precision: 0.8937 - val_loss: 1.8809 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.64383\n",
            "Epoch 15/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.3037 - precision: 0.9188 - val_loss: 1.9599 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.64383\n",
            "Epoch 16/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2710 - precision: 0.8750 - val_loss: 1.7662 - val_precision: 0.5882\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.64383\n",
            "Epoch 17/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2509 - precision: 0.9250 - val_loss: 1.9442 - val_precision: 0.5294\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.64383\n",
            "Epoch 18/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2951 - precision: 0.8750 - val_loss: 1.7054 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.64383\n",
            "Epoch 19/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2404 - precision: 0.8937 - val_loss: 1.8920 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.64383\n",
            "Epoch 20/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2226 - precision: 0.9250 - val_loss: 2.0528 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.64383\n",
            "Epoch 21/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.2261 - precision: 0.9062 - val_loss: 2.3316 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.64383\n",
            "Epoch 22/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1924 - precision: 0.9437 - val_loss: 1.9046 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.64383\n",
            "Epoch 23/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1461 - precision: 0.9562 - val_loss: 1.7241 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.64383\n",
            "Epoch 24/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1323 - precision: 0.9562 - val_loss: 1.8454 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.64383\n",
            "Epoch 25/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1489 - precision: 0.9437 - val_loss: 2.2884 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.64383\n",
            "Epoch 26/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1417 - precision: 0.9500 - val_loss: 2.8554 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.64383\n",
            "Epoch 27/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1220 - precision: 0.9500 - val_loss: 2.6690 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.64383\n",
            "Epoch 28/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1192 - precision: 0.9562 - val_loss: 2.2417 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.64383\n",
            "Epoch 29/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1317 - precision: 0.9562 - val_loss: 2.0031 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.64383\n",
            "Epoch 30/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1229 - precision: 0.9500 - val_loss: 2.0601 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.64383\n",
            "Epoch 31/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1160 - precision: 0.9688 - val_loss: 1.9104 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.64383\n",
            "Epoch 32/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1117 - precision: 0.9688 - val_loss: 2.1230 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.64383\n",
            "Epoch 33/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0853 - precision: 0.9875 - val_loss: 2.2751 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.64383\n",
            "Epoch 34/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0909 - precision: 0.9688 - val_loss: 2.5072 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.64383\n",
            "Epoch 35/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0703 - precision: 0.9750 - val_loss: 2.4259 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.64383\n",
            "Epoch 36/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0648 - precision: 0.9875 - val_loss: 2.2521 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.64383\n",
            "Epoch 37/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0701 - precision: 0.9813 - val_loss: 1.9687 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.64383\n",
            "Epoch 38/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0787 - precision: 0.9688 - val_loss: 3.6701 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.64383\n",
            "Epoch 39/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.1040 - precision: 0.9750 - val_loss: 2.4733 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.64383\n",
            "Epoch 40/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0780 - precision: 0.9625 - val_loss: 2.2472 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.64383\n",
            "Epoch 41/50\n",
            "160/160 [==============================] - 0s 1000us/step - loss: 0.0784 - precision: 0.9813 - val_loss: 2.3053 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.64383\n",
            "Epoch 42/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0534 - precision: 0.9938 - val_loss: 2.4233 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.64383\n",
            "Epoch 43/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0675 - precision: 0.9750 - val_loss: 2.5137 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64383\n",
            "Epoch 44/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0388 - precision: 0.9875 - val_loss: 2.6087 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.64383\n",
            "Epoch 45/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0387 - precision: 1.0000 - val_loss: 2.7800 - val_precision: 0.7059\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.64383\n",
            "Epoch 46/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0333 - precision: 0.9938 - val_loss: 2.9667 - val_precision: 0.6471\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.64383\n",
            "Epoch 47/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0325 - precision: 0.9938 - val_loss: 2.8514 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.64383\n",
            "Epoch 48/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0401 - precision: 0.9938 - val_loss: 2.9271 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.64383\n",
            "Epoch 49/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0310 - precision: 0.9875 - val_loss: 2.8864 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.64383\n",
            "Epoch 50/50\n",
            "160/160 [==============================] - 0s 1ms/step - loss: 0.0508 - precision: 0.9813 - val_loss: 2.8809 - val_precision: 0.7647\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.64383\n",
            "119/119 [==============================] - 1s 8ms/step\n",
            "{'0': {'precision': 0.5042735042735043, 'recall': 1.0, 'f1-score': 0.6704545454545454, 'support': 59}, '1': {'precision': 1.0, 'recall': 0.03333333333333333, 'f1-score': 0.06451612903225806, 'support': 60}, 'accuracy': 0.5126050420168067, 'macro avg': {'precision': 0.7521367521367521, 'recall': 0.5166666666666667, 'f1-score': 0.3674853372434017, 'support': 119}, 'weighted avg': {'precision': 0.7542196365725777, 'recall': 0.5126050420168067, 'f1-score': 0.36493937751053496, 'support': 119}}\n",
            "10\n",
            "--------------- 0 -----------------\n",
            "{'0': {'precision': 0.8, 'recall': 0.6779661016949152, 'f1-score': 0.7339449541284404, 'support': 59}, '1': {'precision': 0.7246376811594203, 'recall': 0.8333333333333334, 'f1-score': 0.7751937984496124, 'support': 60}, 'accuracy': 0.7563025210084033, 'macro avg': {'precision': 0.7623188405797101, 'recall': 0.7556497175141244, 'f1-score': 0.7545693762890264, 'support': 119}, 'weighted avg': {'precision': 0.7620021921812203, 'recall': 0.7563025210084033, 'f1-score': 0.754742690760964, 'support': 119}}\n",
            "--------------- 1 -----------------\n",
            "{'0': {'precision': 0.7543859649122807, 'recall': 0.7288135593220338, 'f1-score': 0.7413793103448276, 'support': 59}, '1': {'precision': 0.7419354838709677, 'recall': 0.7666666666666667, 'f1-score': 0.7540983606557377, 'support': 60}, 'accuracy': 0.7478991596638656, 'macro avg': {'precision': 0.7481607243916242, 'recall': 0.7477401129943503, 'f1-score': 0.7477388355002826, 'support': 119}, 'weighted avg': {'precision': 0.7481084114460724, 'recall': 0.7478991596638656, 'f1-score': 0.7477922768881436, 'support': 119}}\n",
            "--------------- 2 -----------------\n",
            "{'0': {'precision': 0.921875, 'recall': 1.0, 'f1-score': 0.959349593495935, 'support': 59}, '1': {'precision': 1.0, 'recall': 0.9166666666666666, 'f1-score': 0.9565217391304348, 'support': 60}, 'accuracy': 0.957983193277311, 'macro avg': {'precision': 0.9609375, 'recall': 0.9583333333333333, 'f1-score': 0.9579356663131848, 'support': 119}, 'weighted avg': {'precision': 0.961265756302521, 'recall': 0.957983193277311, 'f1-score': 0.9579237845721533, 'support': 119}}\n",
            "--------------- 3 -----------------\n",
            "{'0': {'precision': 0.6666666666666666, 'recall': 0.8813559322033898, 'f1-score': 0.7591240875912408, 'support': 59}, '1': {'precision': 0.8292682926829268, 'recall': 0.5666666666666667, 'f1-score': 0.6732673267326732, 'support': 60}, 'accuracy': 0.7226890756302521, 'macro avg': {'precision': 0.7479674796747967, 'recall': 0.7240112994350283, 'f1-score': 0.716195707161957, 'support': 119}, 'weighted avg': {'precision': 0.7486506797841087, 'recall': 0.7226890756302521, 'f1-score': 0.715834964469274, 'support': 119}}\n",
            "--------------- 4 -----------------\n",
            "{'0': {'precision': 0.8928571428571429, 'recall': 0.847457627118644, 'f1-score': 0.8695652173913044, 'support': 59}, '1': {'precision': 0.8571428571428571, 'recall': 0.9, 'f1-score': 0.8780487804878048, 'support': 60}, 'accuracy': 0.8739495798319328, 'macro avg': {'precision': 0.875, 'recall': 0.873728813559322, 'f1-score': 0.8738069989395546, 'support': 119}, 'weighted avg': {'precision': 0.8748499399759905, 'recall': 0.8739495798319328, 'f1-score': 0.873842644162649, 'support': 119}}\n",
            "--------------- 5 -----------------\n",
            "{'0': {'precision': 0.8775510204081632, 'recall': 0.7288135593220338, 'f1-score': 0.7962962962962963, 'support': 59}, '1': {'precision': 0.7714285714285715, 'recall': 0.9, 'f1-score': 0.8307692307692307, 'support': 60}, 'accuracy': 0.8151260504201681, 'macro avg': {'precision': 0.8244897959183674, 'recall': 0.8144067796610169, 'f1-score': 0.8135327635327635, 'support': 119}, 'weighted avg': {'precision': 0.824043903275596, 'recall': 0.8151260504201681, 'f1-score': 0.8136776077952548, 'support': 119}}\n",
            "--------------- 6 -----------------\n",
            "{'0': {'precision': 0.8679245283018868, 'recall': 0.7796610169491526, 'f1-score': 0.8214285714285715, 'support': 59}, '1': {'precision': 0.803030303030303, 'recall': 0.8833333333333333, 'f1-score': 0.8412698412698413, 'support': 60}, 'accuracy': 0.8319327731092437, 'macro avg': {'precision': 0.8354774156660949, 'recall': 0.831497175141243, 'f1-score': 0.8313492063492064, 'support': 119}, 'weighted avg': {'precision': 0.8352047508540293, 'recall': 0.8319327731092437, 'f1-score': 0.8314325730292118, 'support': 119}}\n",
            "--------------- 7 -----------------\n",
            "{'0': {'precision': 0.7872340425531915, 'recall': 0.6271186440677966, 'f1-score': 0.6981132075471698, 'support': 59}, '1': {'precision': 0.6944444444444444, 'recall': 0.8333333333333334, 'f1-score': 0.7575757575757577, 'support': 60}, 'accuracy': 0.7310924369747899, 'macro avg': {'precision': 0.740839243498818, 'recall': 0.730225988700565, 'f1-score': 0.7278444825614637, 'support': 119}, 'weighted avg': {'precision': 0.7404493712378568, 'recall': 0.7310924369747899, 'f1-score': 0.7280943252086427, 'support': 119}}\n",
            "--------------- 8 -----------------\n",
            "{'0': {'precision': 0.7758620689655172, 'recall': 0.7627118644067796, 'f1-score': 0.7692307692307694, 'support': 59}, '1': {'precision': 0.7704918032786885, 'recall': 0.7833333333333333, 'f1-score': 0.7768595041322314, 'support': 60}, 'accuracy': 0.773109243697479, 'macro avg': {'precision': 0.7731769361221028, 'recall': 0.7730225988700565, 'f1-score': 0.7730451366815003, 'support': 119}, 'weighted avg': {'precision': 0.7731543719805615, 'recall': 0.773109243697479, 'f1-score': 0.7730771901894896, 'support': 119}}\n",
            "--------------- 9 -----------------\n",
            "{'0': {'precision': 0.5042735042735043, 'recall': 1.0, 'f1-score': 0.6704545454545454, 'support': 59}, '1': {'precision': 1.0, 'recall': 0.03333333333333333, 'f1-score': 0.06451612903225806, 'support': 60}, 'accuracy': 0.5126050420168067, 'macro avg': {'precision': 0.7521367521367521, 'recall': 0.5166666666666667, 'f1-score': 0.3674853372434017, 'support': 119}, 'weighted avg': {'precision': 0.7542196365725777, 'recall': 0.5126050420168067, 'f1-score': 0.36493937751053496, 'support': 119}}\n",
            "FINAL\n",
            "{'0': {'precision': 0.7848629938938353, 'recall': 0.8033898305084743, 'f1-score': 0.7818886552909101, 'support': 59}, '1': {'precision': 0.8192379437038181, 'recall': 0.7416666666666666, 'f1-score': 0.7308120468235582, 'support': 60}, 'accuracy': 0.7563025210084033, 'macro avg': {'precision': 0.8020504687988266, 'recall': 0.7725282485875706, 'f1-score': 0.7563503510572341, 'support': 119}, 'weighted avg': {'precision': 0.8021949013610534, 'recall': 0.7722689075630252, 'f1-score': 0.7561357434586318, 'support': 119}}\n",
            "0.8020504687988266 0.7725282485875706 0.7870125984789971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xaf8Fe2xgsyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}